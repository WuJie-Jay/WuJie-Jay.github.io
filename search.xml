<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自动驾驶概要</title>
      <link href="/archives/29d025d8.html"/>
      <url>/archives/29d025d8.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535056564&auto=1&height=66"></iframe></div><h2 id="行业现状"><a href="#行业现状" class="headerlink" title="行业现状"></a>行业现状</h2><p>粗略来说，自动驾驶公司可以分为两大类别：</p><p><strong>一类是传统的车企</strong>（比如国外的大众，宝马，通用，丰田等，国内的长城，吉利等），<strong>新能源车企</strong>（比如特斯拉，蔚来，小鹏等）和<strong>Tier1</strong>（比如国外老牌的博世，大陆，安波福等，以及国内新兴的华为，大疆等）。这类公司的首要目标是量产，一般以L2级别方案为主，目前也在向L3级别扩展。</p><p><strong>另外一类是一些方案提供商或者初创公司</strong>（比如Waymo，Mobileye，Pony.AI，Momenta，TuSimple等）。这些公司致力于发展L4级别的自动驾驶技术，面向的是诸如Robotaxi，Robotruck和Robobus之类的应用。</p><p><strong>对于不同的自动驾驶级别，不同的应用场景，传感器的配置方案也不尽相同。</strong>对于L2级别的应用，比如紧急制动和自适应巡航，可以只采用前视单目摄像头或者前向毫米波雷达。如果需要变道辅助功能，则需要增加传感器对相邻车道进行感知。常用的方案是在车头和车尾增加多个角雷达，以实现360度的目标检测能力。对于L3级别的应用，需要在特定场景下实现车辆的完全自主驾驶，因此需要扩展车辆对周边环境的感知能力。这时就需要增加激光雷达，侧视和后视的摄像头和毫米波雷达，以及GPS，IMU和高精度地图来辅助车辆定位。到了L4级别以后，由于在特定场景下不需要人工接管了，传感器就不仅需要高精确度，还需要高可靠性。这就需要增加传感器的冗余性，也就是说需要备用系统。</p><h2 id="概念及研究内容"><a href="#概念及研究内容" class="headerlink" title="概念及研究内容"></a>概念及研究内容</h2><p>自动驾驶车是一种通过计算机系统实现无人驾驶的智能汽车，是一个集合多领域先进技术的综合产物。<strong>感知、决策、执行</strong>是自动驾驶车的三大核心，使车辆能够感知和理解行驶环境，进行主动决策，对全局或局部地图进行实时地路径规划，并准确控制车辆运动，跟踪期望的轨迹，达到期望的行驶要求。</p><p>在智能车辆的研究中，主要涉及到以下的几个问题：（1）环境传感感知：该部分的主要作用是通过传感器识别车辆周围的障碍物与环境，建立车辆周围环境的三维场景；（2）轨迹规划与决策：该部分主要是在获得车辆周围环境后规划与决策出一条最优的行驶轨迹；（3）轨迹跟随控制：该部分主要是通过控制车辆的转向系统以及驱动制动系统使得车辆按照期望的轨迹行驶，包括了纵向的速度跟随控制以及侧向的路径跟随控制。</p><img src="E:\00近期\自动驾驶系统基本构成.png" alt="自动驾驶系统基本构成" style="zoom: 80%;" /><p>​                                                                        <strong>图a  自动驾驶系统基本构成</strong></p><h3 id="环境传感感知"><a href="#环境传感感知" class="headerlink" title="环境传感感知"></a>环境传感感知</h3><p>在自动驾驶赛道中，感知的目的是为了模仿人眼采集相关信息，为后续做决策提供必要的信息。根据所做决策的任务不同，感知可以包括很多子任务：如<strong>车道线检测</strong>、<strong>3D目标检测</strong>、<strong>障碍物检测</strong>、<strong>红绿灯检测</strong>等等；再根据感知预测出的结果，完成决策；最后根据决策结果执行相应的操作（如<strong>减速、停车、变道、超车</strong>等）。</p><p>环境感知是实现自动驾驶的第一环节，也是最为关键的一个环节。车辆通过各类传感器，例如<strong>摄像头</strong>、<strong>毫米波雷达</strong>、<strong>超声波雷达</strong>、<strong>激光雷达</strong>等获取周边信息，产生图片数据、视频数据、点云图像、电磁波等信息，去除噪点信息后利用不同类型数据形成冗余同时提升感知精度，绘制周围区域的高精度3D地图。摄像头、雷达和激光雷达三者是相辅相成的关系。其中，<strong>摄像头</strong>是感知系统中最常用的传感器，优势在于能够提取丰富的纹理和颜色信息，因此适用于目标的分类。但是其缺点在于对于距离的感知能力较弱，并且受光照条件影响较大。<strong>激光雷达</strong>在一定程度上弥补了摄像头的缺点，可以精确的感知物体的距离和形状，因此适用于中近距的目标检测和测距。但是其缺点在于成本较高，量产难度大，感知距离有限，而且同样受天气影响较大。<strong>毫米波雷达</strong>具有全天候工作的特点，可以比较精确的测量目标的速度和距离，感知距离较远，价格也相对较低，因此适用于低成本的感知系统或者辅助其它的传感器。但是缺点在于高度和横向的分辨率较低，对于静止物体的感知能力有限。</p><p>![](E:\00近期\2021年 1-5月国内新发布车型传感器配置及核心功能.png)</p><p>​                                                <strong>图b  2021年 1-5月国内新发布车型传感器配置及核心功能</strong></p><p>从硬件层面来看，对于不同级别自动驾驶汽车和驾驶任务而言，需要的传感器类型、数量和性能也有所区别。从软件算法层面来看，主要有视觉和点云的技术方案，比如特斯拉为代表的纯视觉；Waymo为代表的以点云和视觉融合的方案；还有以视觉为主融合激光点云的方案。</p><p>环境感知系统的硬件基础是多种传感器以及它们的组合，而软件方面的核心则是感知算法。总的来说，<strong>感知算法要完成两个主要的任务：物体检测和语义分割</strong>。前者得到的是场景中重要目标的信息，包括位置，大小，速度等，是一种稀疏的表示；而后者得到的是场景中每一个位置的语义信息，比如可行驶，障碍物等，是一种稠密的表示。<strong>这两个任务的结合被称为全景分割</strong>，这也是自动驾驶和机器人领域最近兴起的一个概念。对于物体目标（比如车辆，行人），全景分割输出其分割Mask，类别和实例ID；对于非物体目标（比如道路，建筑物），则只输出其分割Mask和类别。<strong>环境感知系统的终极目标就是要得到车辆周边三维空间中全景分割结果。</strong></p><p>自动驾驶驾驶技术这一轮的爆发很大程度上来源于深度学习在计算机视觉领域取得的突破，而这个突破首先是从图像分类和图像中的物体检测开始的。在自动驾驶环境感知中，深度学习最先取得应用的任务是<strong>单张二维图像中的物体检测。</strong>这个领域中的经典算法，比如Faster R-CNN，YOLO，CenterNet等都是不同时期视觉感知算法的主流。但是，车辆不能仅仅依靠一张二维图像上的检测结果来行驶。因此，为了满足自动驾驶应用的需求，这些基础的算法还需要进行进一步的扩展，其中最重要的就是<strong>融合时序信息和三维信息</strong>。前者衍生出了<strong>物体跟踪算法</strong>，后者衍生出了<strong>单目/双目/多目的三维物体检测算法</strong>。以此类推，语义分割包含了<strong>图像语义分割</strong>，<strong>视频语义分割</strong>，<strong>稠密深度估计</strong>。</p><p>为了得到更加精确的三维信息，激光雷达也一直是自动驾驶感知系统的重要组成部分，尤其是对于L3/4级别的应用。<strong>激光雷达的数据是相对稀疏的点云</strong>，这与图像稠密的网格结构差别非常大，因此图像领域常用的算法需要经过一定的改动才能应用到点云数据。<strong>点云感知的任务也可以按照物体检测和语义分割来划分，前者输出三维的物体边框，而后者输出点云中每个点的语义类别。</strong>为了利用图像领域的算法，点云可以转换为鸟瞰视图（Bird’s Eye View）或者前视图（Range View）下的稠密网格结构。此外，也可以改进深度学习中的卷积神经网络（Convolutional Neural Network, CNN），使其适用于稀疏的点云结构，比如PointNet或者Graph Neural Network。</p><p>毫米波雷达由于其全天候工作，测速准确，以及低成本的特点，也被广泛的用于自动驾驶感知系统中，不过一般应用在L2级别的系统中，或者在L3/4级系统中作为其它传感器的辅助。<strong>毫米波雷达的数据一般来说也是点云</strong>，但是比激光雷达的点云更为稀疏，空间分辨率也更低。<strong>相比于摄像头和激光雷达，毫米波雷达的数据密度非常低</strong>，因此一些传统方法（比如聚类和卡尔曼滤波）表现的并不比深度学习差很多，而这些传统方法的计算量相对较低。最近几年来，开始有研究者<strong>从更底层的数据出发，用深度学习代替经典的雷达信号处理，通过端对端的学习取得了近似激光雷达的感知效果</strong>。</p><p>单个传感器的感知能力总是有限的，如果把系统成本先放在一边，多传感器融合的方案自然更好的选择。一般来说，摄像头是感知系统的必备的传感器，为了得到深度信息和360度的视场，可以采用<strong>双目或者多目融合的方案</strong>。为了更准确的获得三维和运动信息，<strong>摄像头也可以与激光雷达和毫米波雷达进行融合</strong>。这些传感器的坐标系不同，数据形式不同，甚至采集频率也不同，因此融合算法的设计并不是一件简单的任务。粗略来说，<strong>融合可以在决策层（融合不同传感器的输出）或者数据层（融合不同传感器的数据或者中间结果）来进行</strong>。数据层融合理论上说是更好的方法，但是对<strong>传感器之间的空间和时间对齐</strong>要求会更高。</p><h3 id="轨迹规划与决策"><a href="#轨迹规划与决策" class="headerlink" title="轨迹规划与决策"></a>轨迹规划与决策</h3><p><strong>规划（planning）</strong>承接环境感知，并下启车辆控制。其规划出来的轨迹是带速度信息的路径。广义上，规划（planning）可分为<strong>路由寻径（Routing）、行为决策（Behavioral Decision）、运动规划（Motion Planning）</strong>。路由寻径：是全局路径规划，可简单的理解为传统地图导航+高精地图（包含车道信息和交通规则等）；行为决策：决策车辆是否跟车、在遇到交通灯和行人时的等待避让、以及路口和其他车辆的交互通过；运动规划：是局部路径规划，是无人车未来一段时间内的期望行驶路径，需满足汽车运动学、动力学、舒适性和无碰撞等要求。</p><p>轨迹规划的任务是计算出一个无碰撞可执行的轨迹（包含路径和速度信息），保证车辆从起点安全的驾驶到目的地，并尽可能高效。其问题的本质是一个多目标的数学优化问题。路径规划的底层数学理论主要是“最优化理论“”矩阵理论“”数值分析“，构建目标函数与约束条件同时求极值来得到最优控制量（路径）这个套路来自于”最优化理论“；在求解最优控制量时大家常见的牛顿法、最速下降法等等这些数值求解方法，本质来自于数值求解代数等式方程，属于”数值分析“；求解过程中所见到的导数雅可比矩阵、判断条件中的向量范数等等，本质就是把一维数值求解放到了高维，升维的方式由“矩阵理论”研究。</p><p>根据数学定义，横向、纵向规划均为非凸优化问题。横向规划，即s→(x,y)，决定了轨迹的形状。纵向规划是t→s，是指在此形状上运动的速度状态，也就是时间与位移的关系。横向规划和纵向规划联合起来就是t→(x,y)。</p><p><strong>决策（Decision）</strong>使轨迹规划化繁为简。既然轨迹规划是非凸优化问题，我们需要利用决策模块来解决这个问题。从数学上来讲，决策就是为了限定非凸问题（轨迹规划）的解空间，将问题转化为凸的。</p><p>主要的优化目标包括：</p><p><strong>安全性：</strong>避免与场景中的障碍物发生碰撞；针对动态障碍物，由于其未来运动的不确定性，降低其未来的碰撞风险；</p><p><strong>稳定性：</strong>由于车辆的惯性较大，灵活性差，期望轨迹需要保证车辆的物理可行性和控制器的稳定性；</p><p><strong>舒适性：</strong>考虑到乘员的舒适性，需要在满足安全性和稳定性的同时保证车辆的驾驶舒适度，包括加减速以及转向等过程；</p><p><strong>驾驶效率：</strong>在满足安全性和稳定性的同时，保证车辆以更快的速度驾驶，从而更短的时间到达目的地。</p><p>在实际场景中，规划过程需要考虑各种物理约束，有且不限于：</p><p><strong>加减速度约束：</strong>受到动力系统和制动系统的性能极限，及驾驶员的安全性和舒适性的制约；</p><p><strong>非完整性约束：</strong>车辆具有三个运动自由度，但是只有两个控制自由度，其非完整性约束决定了轨迹的物理可行性；</p><p><strong>动力学约束：</strong>考虑到车辆的动力学特性和车身稳定性，其驾驶过程中的曲率和横摆角速度具有一定的约束；</p><p>当然，在工程中，解决特定场景的规划问题，可以进行一定程度的简化。下面是几种应用较为广泛的算法。</p><p>​                                                                                <strong>表1  轨迹规划算法</strong></p><p><img src="E:\00近期\几种应用较为广泛的路径规划算法.jpg"></p><p>例如，百度的Apollo决策规划算法是一种<strong>数值优化的算法</strong>，且是“轻决策重规划”的方案，一共有3种Planner解决方案。</p><p>​                                                                        <strong>表2  三种Planner解决方案</strong></p><table><thead><tr><th><strong>EM Planner</strong></th><th><strong>路径和速度解耦，分开规划再融合</strong></th></tr></thead><tbody><tr><td>RTK Planner</td><td>基于录制的轨迹来规划行车的路线</td></tr><tr><td>Lattice Planner</td><td>路径和速度一起规划，直接输出高维的轨迹</td></tr></tbody></table><p>决策规划是智能汽车导航和控制的基础，从轨迹决策的角度考虑的，可分为<strong>全局规划和局部规划两个层次</strong>。其中，全局路径规划的任务是根据全局地图数据库信息规划出自起始点至目标点的一条无碰撞、可通过的路径。由于全局路径规划所生成的路径只能是从起始点到目标点的粗略路径，并没有考虑路径的方向、宽度、曲率、道路交叉以及路障等细节信息，加之智能汽车在行驶过程中受局部环境和自身状态的不确定性的影响，会遇到各种不可测的情况。因此，在智能汽车的行驶过程中，必须以局部环境信息和自身状态信息为基础，规划出一段无碰撞的理想局部路径，这就是局部路径规划。</p><h4 id="全局规划方法"><a href="#全局规划方法" class="headerlink" title="全局规划方法"></a><strong>全局规划方法</strong></h4><p><strong>(1) 基于状态空间的最优控制轨迹规划方法</strong></p><p><strong>最优控制一般包括一到两个性能指标</strong>，对于控制变量的取值不受约束的情况，一般用变分法进行求解；对于控制量受约束的情况，一般用极小值原理进行求解。</p><p><strong>(2)基于参数化曲线的轨迹规划方法</strong></p><p>B 样条曲线由一组称作控制点的向量来确定，这些控制点按顺序连接形成一个控制多边形，B样条曲线就是逼近这个控制多边形。<strong>通过确定控制点的位置，可以控制曲线的形状。</strong></p><p><strong>(3)基于基于系统特征的轨迹规划方法</strong></p><p>微分平坦法是基于系统特征的一种轨迹规划方法。微分平坦是指可以找到一组系统输出，使得所有状态变量和输入变量都可以由这组输出及其导数决定(不需积分)。</p><h4 id="局部规划方法（也可称之为实时路径规划）"><a href="#局部规划方法（也可称之为实时路径规划）" class="headerlink" title="局部规划方法（也可称之为实时路径规划）"></a><strong>局部规划方法（也可称之为实时路径规划）</strong></h4><p><strong>智能汽车进行局部路径规划（也可称之为实时路径规划）</strong>，一般是指在有障碍物的环境中，如何利用自身传感器感知周边环境，并寻找一条从当前点到目标点点的局部行驶路径，使智能汽车在本次任务中能安全快速地到达目标位置。局部路径规划的方法主要包括以下两个关键部分：</p><p><strong>建立环境模型，</strong>即将智能汽车所处现实世界抽象后，建立计算机可认知的环境模型；</p><p><strong>搜索无碰路径</strong>，即在某个模型的空间中，在多种约束条件下，选择合乎条件的路径搜索算法。根据不同行驶环境的特点，智能汽车局部路径规划中的侧重点和难点都会有相应不同：</p><ul><li><p>在高速公路中，行车环境比较简单但车速较快，此时对智能汽车控制精度要求很高，算法难点主要在于环境信息获取的位置精度和路径搜索的速度；</p></li><li><p>在城市半结构化道路中，道路环境特征性比较明显但交通环境比较复杂，周边障碍物较多，这就对智能汽车识别道路特征和障碍物的可靠性有较高要求，路径规划的难点主要在于车辆周边环境建模和避障行驶的路径搜索，特别是对动态障碍物方向和速度预测；</p></li><li><p>在越野环境的非结构化道路中，智能汽车所处的环境没有明显的道路边界，路面起伏不平，可能有大坑或土堆，这就对智能汽车识别周围环境，特别是地形地势有较高要求，路径规划的难点主要在于车辆可通行区域的识别。</p></li></ul><p><strong>(1)基于滚动时域优化的轨迹规划方法</strong></p><p>该方法能够确保机器人在未知环境中安全地避开障碍物行驶，具有反应速度快的优点，能够迅速适应变化的环境，是一种有效实用的工具，但计算量相对较大。</p><p><strong>(2)基于轨迹片段的运动规划方法</strong></p><p>轨迹片段包含配平轨迹和机动轨迹。其中配平轨迹是系统处于相对平衡时所经历的轨迹，而机动轨迹则是系统从一个相对平衡跃入另外一个相对平衡所经历的轨迹。可以通过考虑车辆的运动学和动力学约束条件，基于最优控制原理的机动轨迹设计方法和随机采样法，实现基于轨迹片段连接的最优运动轨迹规划和快速运动规划。但是该方法计算较为复杂，使其在实际应用中受到限制。</p><p><strong>(3)路权分配技术</strong></p><p>路权与车速强相关，可分为期望路权和实际路权，当两者不一致时，就需要进行调节来解决冲突。自主驾驶是智能汽车在任意时刻对路权的检测和使用，多车交互是车群在任意时刻对路权的竞争、占有、放弃等协同过程。<strong>自主驾驶的不确定性，体现在车辆行驶中拥有的路权在不停地发生变化。</strong></p><h3 id="轨迹跟随控制与执行"><a href="#轨迹跟随控制与执行" class="headerlink" title="轨迹跟随控制与执行"></a>轨迹跟随控制与执行</h3><p>自动驾驶控制执行系统是指系统做出决策规划以后，替代驾驶员对车辆进行控制，反馈到底层模块执行任务。可以说，执行控制系统是自动驾驶汽车行驶的基础，车辆的各个操控系统需要通过总线与决策系统相连接，并能够按照决策系统发出的总线指令精确地控制加速程度、制动程度、转向幅度、灯光控制等驾驶动作，以实现车辆的自主驾驶。</p><h4 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h4><p>自动驾驶控制执行的核心技术主要包括车辆的<strong>纵向控制</strong>和<strong>横向控制</strong>技术。纵向控制，即车辆的驱动与制动控制，是指通过对油门和制动的协调，实现对期望车速的精确跟随。横向控制，即通过方向盘角度的调整以及轮胎力的控制，实现自动驾驶汽车的路径跟踪。</p><p><strong>车辆纵向控制</strong>是指对车速以及本车与前后车或障碍物距离的自动控制。自动驾驶汽车采用油门和制动综合控制的方法来实现对预定车速的跟踪，各种电机-发动机-传动模型、汽车运行模型和刹车过程模型与不同的控制算法相结合，构成了各种各样的纵向控制模式。</p><p><img src="E:\00近期\典型的纵向控制系统结构.png"></p><p>​                                                                        <strong>图c  典型的纵向控制系统结构</strong>（虚线框）</p><p><strong>车辆横向控制</strong>指垂直于运动方向上的控制，即转向控制。横向控制系统目标是控制汽车自动保持期望的行车路线，并在不同的车速、载荷、风阻、路况下均有很好的乘坐舒适性和稳定性。</p><p><img src="E:\00近期\典型的横向控制系统结构.png"></p><p>​                                                                        <strong>图d  典型的横向控制系统结构</strong>（虚线框）</p><p>车辆横向控制大致可以分为两种基本设计方法：<strong>基于驾驶员模拟</strong>的控制方法和<strong>基于车辆动力学模型</strong>的控制方法。</p><p>基于驾驶员模拟的方法又可以详细划分为两种，一种是使用较简单的动力学模型和驾驶员操纵规则设计控制器，另一种是用驾驶员操纵过程的数据训练控制器获取控制算法。</p><p>基于车辆动力学模型的方法，需要建立较精确的汽车横向运动模型。典型模型如单轨模型，该模型认为汽车左右两侧特性相同。</p><h4 id="控制算法"><a href="#控制算法" class="headerlink" title="控制算法"></a>控制算法</h4><p>自动驾驶控制方法可划分为两种，分别为传统控制方法与智能控制方法。</p><h5 id="传统控制方法"><a href="#传统控制方法" class="headerlink" title="传统控制方法"></a>传统控制方法</h5><p>传统控制方法主要有<strong>PID 控制</strong>、<strong>模糊控制</strong>、<strong>最优控制</strong>、**滑模控制(预测模型控制MPC)**等。</p><p>PID控制，又称之为比例积分微分控制，是最早发展起来的控制策略之一。其原理简单来说，是根据给定值和实际输出值构成控制偏差，将偏差按比例、积分和微分通过线性组合构成控制量，对被控对象进行控制。由于其算法简单、鲁棒性好和可靠性高，至今仍有90%左右的控制回路具有PID结构。</p><p>模糊控制，全称为模糊逻辑控制，是以模糊集合论、模糊语言变量和模糊逻辑推理为基础的一种计算机数字控制技术。与经典控制理论相比，模糊逻辑控制策略最大的特点是不需要准确的数学公式来建立被控对象的精确数学模型，因此可极大简化系统设计和数学建模的复杂性，提高系统建模和仿真控制的效率。不过，模糊控制的设计缺乏系统性，对复杂系统的控制是存在一定问题。</p><p>最优控制着重于研究使控制系统的性能指标实现最优化的基本条件和综合方法。可以概括为：对一个受控的动力学系统或运动过程，从一类允许的控制方案中找出一个最优的控制方案，使系统的运动在由某个初始状态转移到指定的目标状态的同时，其性能指标值为最优。</p><p>滑模控制也叫变结构控制，本质上是一类特殊的非线性控制。该控制策略与其他控制的不同之处在于系统“结构”不固定，可以在动态过程中根据系统当前的状态有目的地不断变化，迫使系统按照预定“滑动模态”的状态轨迹运动。由于滑动模态可以进行设计且与对象参数及扰动无关，因此滑动控制具有快速响应、对应参数变化及扰动不灵敏、无需系统在线辨识、物理实现简单等优点。不过，滑动控制也并不是全无缺点。在实际应用中，当状态轨迹到达滑动模态面后，难以严格沿着滑动模态面向平衡点滑动，而是在其两侧来回穿越地趋近平衡点，这会产生震动影响正常应用。</p><h5 id="智能控制方法"><a href="#智能控制方法" class="headerlink" title="智能控制方法"></a>智能控制方法</h5><p>智能控制方法与传统的控制方法最大的不同在于，智能控制方法更关注于控制对象模型的运用和综合信息学习运用。常见的智能控制方法主要有<strong>基于模型的控制</strong>、<strong>神经网络控制</strong>和<strong>深度学习方法</strong>等。</p><p>基于模型的控制，一般称为模型预测控制，它的当前控制动作是在每一个采样瞬间通过求解一个有限时域开环最优控制问题而获得。其基本原理可概括为：在每个采样时刻，根据当前获得的当前测量信息，在线求解一个有限时域的开环优化问题，并将得到的控制序列的第一个元素作用于被控对象，在一个采样时刻，重复上述过程，再用新的测量值刷新优化问题并重新求解。这种控制方法的优点是对模型的精度要求不高，建模方便，且因为采用非最小化描述的模型，系统鲁棒性、稳定性较好。</p><p>神经网络控制，可以把控制问题看成模式识别问题，被识别的模式映射成“行为”信号的“变化”信号。神经控制最显著的特点是具有学习能力。它是通过不断修正神经元之间的连接权值，并离散存储在连接网络中来实现的。它对非线性系统和难以建模的系统的控制具有良好效果。</p><p>深度学习方法，可以获得深层次的特征表示，免除人工选取特征的繁复冗杂和高维数据的维度灾难等问题，在特征提取与模型拟合方面具有很大优势。由于自动驾驶系统需要尽量减少人的参与，因此深度学习自动学习状态特征的能力，让深度学习在自动驾驶系统的研究中更具优势。</p><h4 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h4><p>根据从行驶环境到驾驶动作的映射过程，自动驾驶控制技术可以分为间接控制和直接控制两种不同方案。</p><p><strong>基于规划 - 跟踪的间接控制方法</strong></p><p>自动驾驶间接控制方案可以简单概括为，根据当前车辆行为需求，在满足车辆自身运动学和动力学约束条件下规划出一条空间上可行且时间上可控的无碰撞安全运动轨迹，然后设计适当的控制律跟踪生成的目标轨迹，从而实现自主驾驶。</p><p>![](E:\00近期\基于规划- 跟踪的间接控制方法.png)</p><p>​                                                                        <strong>图e  间接控制结构示意图</strong></p><p><strong>基于人工智能的直接控制方法</strong></p><p>由于自动驾驶汽车行驶环境具有不确定性、不可重复性和不可预测性等特征，很难建立精确的数学模型进行控制律的设计，因此传统控制策略已无法满足自动驾驶驾驶控制的要求。</p><p>这样的背景下，基于人工智能的直接控制方法就成为目前自动驾驶控制系统的主流形式。</p><p><img src="E:\00近期\直接控制结构示意图.png"></p><p>​                                                                        <strong>图f  直接控制结构示意图</strong></p><p>基于人工智能决策控制模型本质上是模拟人脑对外界环境信息和车体本身信息的感知，同时由驾驶经验并同在线学习机制来获得持续稳定输出的过程。</p><p>这种控制模式可以有效提升自动驾驶汽车在面对不同场景下的随机应变能力，代表着自动驾驶控制执行系统在未来一段时间内的主流发展方向。</p><hr><h2 id="引申与思考"><a href="#引申与思考" class="headerlink" title="引申与思考"></a>引申与思考</h2><p>近年来，应用于计算机视觉领域的深度学习技术得到了突破性的发展，随即被广泛应用于了自动驾驶领域。有大量的自动驾驶系统，包括 Apollo，Autoware等，使用了诸如物体检测、目标追踪、姿态预测等深度学习相关技术。深度学习的应用使得自动驾驶系统能够更加适应复杂的环境条件，更加适应复杂的任务需求；使自动驾驶逐渐从实验室走向现实，从辅助驾驶过渡到全自动驾驶。然而在另一方面，自动驾驶系统层出不穷的安全性问题引发了人们的担忧。深度学习系统自身具有的高度非线性、不可解释性特征，给系统的安全性分析提供了困难。目前已有大量的关于基于深度学习的自动驾驶系统的安全性保障、安全性测试的工作被提出，包括神经元覆盖、场景迁移覆盖、输入空间划分、故障注入检测、测试数据集生成等。</p><p>未来的挑战主要有几方面：</p><p>首先，在当今生物计算、脑科学尚未全面解锁的情况下，实证主义以“机器学习+大数据/复杂环境+大算力”模式训练大规模智能模型，确实可以解决不少问题，但是数据也面临着不少问题。首先数据需要大量的人力标注，人力标注意味着人也会出错，一旦标注错误，如何高效的发现错误将是一个关键的点，或者在标注的时候带有某些不和谐的色彩（性别、肤色等等），这都会影响算法模型的准确性公平公正性。</p><p>其次，算法模型的验证一般会采用开放数据集，但是这些数据集有一定的局限性，例如结构化场景的局限，Cornercase的不足等等，当模型精度到达一定的程度后就需要新的数据去训练和验证，那么这些数据的来源无非是通过实际测试或者虚拟仿真，虚拟仿真的工具开发将会是重点之一，这将会是进入数字世界的一把钥匙。</p><p>最后，算法再厉害也存在物理的限制，无论是哪种传感器都会有物理限制。根据信息论的观点，对抗不确定性的方法就是获取更多的相关信息。自动驾驶感知面对各种不确定，在硬件方面、软件方面同步获取信息，未来产品边界成熟在成本的压力下，将会选择最优化的方案，基于物理性质的感知将会是下一个重点方向，这就是汽车、手机行业开始跨界融合的原因之一。在视觉领域，无论是摄像头物理尺寸还是底层防抖、去反射、去雾等等，手机厂商已经有相对成熟的技术方案，随着汽车智能化的不断普及，手机行业和汽车行业的技术将会不断融合，具体就看融合的方式方法。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoboMaster视觉组学习整理（1）</title>
      <link href="/archives/b13dfb3f.html"/>
      <url>/archives/b13dfb3f.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=530397462&auto=1&height=66"></iframe></div>前面序章（0）提到了代码框架，我觉得直接按照框架顺序讲不便于梳理逻辑，所以打算按照算法流程来讲解各部分代码。<h1 id="算法流程介绍"><a href="#算法流程介绍" class="headerlink" title="算法流程介绍"></a>算法流程介绍</h1><p>先看下流程图：</p><img src="D:\MyBlog\MyBlog\source\img\算法流程图.png" alt="" style="zoom:50%;" /><p>（未完待续。。。）</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoboMaster视觉组学习整理（0）</title>
      <link href="/archives/25519.html"/>
      <url>/archives/25519.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=530397462&auto=1&height=66"></iframe></div><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>上学期一直在请教大佬和自学代码的路上，没有什么产出，想着这学期自己写点东西，就从RoboMaster这个比赛写起吧。我是研一上学期参加的比赛，做视觉识别部分。当时的想法很简单，因为很喜欢Linux，个人不太想调电控和焊板子接线，就参加了视觉部分。由于本科专业以及之前工作的关系，其实更熟悉机械结构设计，学视觉主要是因为兴趣，还有想换个方向。做视觉可以开发一些很有趣的东西，学到很多前沿的知识。</p><p>来到新战队之后百废待兴，可以说战队管理工作是我一手带起来的。今年我担任战队项目管理（兼任视觉组长）期间，也发现了一些现实问题：</p><p>一个是队员基础能力弱和经验缺乏带来的队伍上限不高。上学期各组真的是从零开始的——没技术没经验。虽然队里之前有研究生做，但是却没有相关技术传承，连个真正能解答问题的人都没有。例如上届视学组名义上是有几个人的，但由于各种原因都跑路了，只有一位学姐偶尔能说上话，但是很多时候她也无法及时帮忙解决问题。所以我很能理解一些新接触RM视觉的学弟学妹心中的迷茫；</p><p>一个是人员稀缺，目前视觉组只有我和另外一名研究生（之前还有个本科学弟的，前不久因为个人原因退队了……），现阶段再招人也不现实，而且参赛名额有限，所以暂时由我负责两个兵种。希望换届的时候，梯队这边能留下足够多人数；</p><p>所以我打算将RoboMaster视觉的各个方面的技术要点写成一系列的教程，帮助新手入门，也希望对其他战队有一点参考价值。教程代码主要参考了南理工本部Alliance战队上个赛季代码（感谢磊哥），传送门：<a href="https://github.com/shilei31415/AutoShot">https://github.com/shilei31415/AutoShot</a> 。本部往届的代码写得不错，搭建了一个很好的项目框架，很感谢他们能分享出来，帮助我们新队伍更快成型。</p><p>下面是代码框架，可以先看下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AutoShot</span><br><span class="line"> ├── AutoShot//识别装甲</span><br><span class="line"> │   ├── include//.h文件</span><br><span class="line"> │   │   ├── ANN.h//数字识别</span><br><span class="line"> │   │   ├── Armor.h//Mark匹配</span><br><span class="line"> │   │   ├── Mark.h//识别Mark</span><br><span class="line"> │   │   ├── pnp.h//PnP测距</span><br><span class="line"> │   │   ├── TargetFind.h//调用Armor和Mark</span><br><span class="line"> │   │   └── trajectory.h//考虑空气阻力的抛物线(未使用)</span><br><span class="line"> │   └── src//.cpp文件</span><br><span class="line"> │       ├── ANN.cpp</span><br><span class="line"> │       ├── Armor.cpp</span><br><span class="line"> │       ├── Mark.cpp</span><br><span class="line"> │       ├── pnp.cpp</span><br><span class="line"> │       ├── TargetFind.cpp</span><br><span class="line"> │       └── targetFinderTest.cpp//测试targetFinder</span><br><span class="line"> ├── CMakeLists.txt</span><br><span class="line"> ├── HeroVision.h//常用宏定义</span><br><span class="line"> ├── include.h//常用头文件</span><br><span class="line"> ├── main.cpp</span><br><span class="line"> ├── others//其他功能</span><br><span class="line"> │   ├── Calibrator//标定</span><br><span class="line"> │   │   ├── Calibrate.cpp</span><br><span class="line"> │   │   ├── Calibrator.cpp</span><br><span class="line"> │   │   ├── Calibrator.h</span><br><span class="line"> │   │   ├── cr</span><br><span class="line"> │   │   └── ip</span><br><span class="line"> │   ├── camera//相机驱动</span><br><span class="line"> │   │   ├── CameraApi.h</span><br><span class="line"> │   │   ├── CameraDefine.h</span><br><span class="line"> │   │   ├── CameraStatus.h</span><br><span class="line"> │   │   ├── MindVisionCamera.cpp</span><br><span class="line"> │   │   └── MindVisionCamera.h</span><br><span class="line"> │   ├── JLink//自己编写的调试助手,显示变量随时间的变化</span><br><span class="line"> │   │   ├── JLink.cpp</span><br><span class="line"> │   │   └── JLink.h</span><br><span class="line"> │   ├── picture//程序运行时保存的图片</span><br><span class="line"> │   ├── SerialPort//串口通信</span><br><span class="line"> │   │   ├── SerialPort.cpp</span><br><span class="line"> │   │   ├── SerialPort.h</span><br><span class="line"> │   │   ├── SerialPortTest.cpp</span><br><span class="line"> │   │   └── serial.sh</span><br><span class="line"> │   └── video//程序运行时保存的视频</span><br><span class="line"> ├── README.md</span><br><span class="line"> └── tools//运行及调试时常用函数</span><br><span class="line">     ├── excption//catch excption并保存到txt (未使用)</span><br><span class="line">     │   ├── myExcption.cpp</span><br><span class="line">     │   └── myExcption.h</span><br><span class="line">     ├── MathTool//常用数学函数</span><br><span class="line">     │   └── MathTool.h</span><br><span class="line">     ├── setter//读取参数</span><br><span class="line">     │   ├── model.pb</span><br><span class="line">     │   ├── para.yaml</span><br><span class="line">     │   ├── setter.cpp</span><br><span class="line">     │   └── setter.h</span><br><span class="line">     └── time//计算帧率,控制帧率</span><br><span class="line">         ├── timer.cpp</span><br><span class="line">         └── timer.h</span><br></pre></td></tr></table></figure><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p><strong>在讲代码之前先解答一些大家可能会有的疑问：</strong></p><h2 id="为什么要用Ubuntu系统"><a href="#为什么要用Ubuntu系统" class="headerlink" title="为什么要用Ubuntu系统"></a>为什么要用Ubuntu系统</h2><p><strong>跟 win 相比：</strong></p><ul><li>免费，安全；</li><li>很多软件可以通过 ppa 源下载，不需要跑到网站上下载，比较方便；</li><li>一般的 server 都使用 linux，所以省去了很多开发兼容性的问题；</li><li>shell 厉害；</li><li>占用空间比较小,可以安装在许多硬件上(TX2/NUC)；</li><li>运行速度比Windows快，运行程序时不像Windows需要调用其他许多东西，同样的程序Ubuntu上运行速度比Windows快10%左右；</li><li>权限高，Ubuntu上<strong>不存在</strong>卸不掉的流氓软件，知道密码甚至可把Ubuntu整个删掉;</li><li>……</li></ul><p><strong>跟其他 linux 相比：</strong></p><ul><li>UI 比较好看；</li><li>文档比较多，用的人比较多，有问题可以一起讨论；</li><li>针对 ubuntu 开发的软件比较多；</li><li>……</li></ul><h2 id="关于ubuntu的建议"><a href="#关于ubuntu的建议" class="headerlink" title="关于ubuntu的建议"></a>关于ubuntu的建议</h2><p>①在自己电脑安装Ubuntu时尽量多分配一些空间，用作平时学习建议至少50G。当然了，如果个人有开发软件的需求，或是跑大型项目、涉及很多图像视频识别时，应尽量分配更多空间，以便不时之需。</p><p>②安装的时候语言建议选择英文，可锻炼英语读写能力，也是因为之后的文件路径默认英文，使用中文路径容易出错。其实安装完成后可以修改语言，但要注意路径保持英文。</p><p>③基本命令要会，不用会太多，“增删改查”及一些常用命令就够了，另外记得安装完系统先换源 <a href="https://www.yisu.com/ask/5536.html">ubuntu更新源是什么意思 - 问答 - 亿速云 (yisu.com)</a> 。<br>“增”可以用命令新建文件、文件夹；<br>“删”可以用命令删除文件、清空文件夹；<br>“改”可以用命令修改文件权限、移动文件；<br>“查”可以用命令查看文件夹中的内容；<br>还有一些脚本语言(shell)，你起码可以用脚本让你的程序实现”开机自启动””程序意外终止后重启”；<br>其他还用很多需要掌握的，建议开始的时候时间较为充裕，尽量用命令，而不是GUI；</p><p>④当你配置完环境，装好程序，马上把你的系统转为镜像文件（虚拟机的话直接用快照功能即可），防止有一天“一切归0”</p><p>⑤最初上手可以安装Ubuntu16。因为16版本的教程比较多，当你熟悉后可以考虑使用18.04或者20.04（这两个版本相关的内容较少，如果报错你可能较难找到解决方法）</p><h2 id="配置OpenCV"><a href="#配置OpenCV" class="headerlink" title="配置OpenCV"></a>配置OpenCV</h2><h3 id="安装-OpenCV-contrib"><a href="#安装-OpenCV-contrib" class="headerlink" title="安装 OpenCV_contrib"></a>安装 OpenCV_contrib</h3><p>OpenCV3.0以上的版本，把一些不稳定的函数放到了第三方库OpenCV_contrib中（比如sift,surf等），为了应用这些功能，我们需要将Contrib中的功能重新加入Opencv。关于OpenCV_contrib，可以参见官方说明 <a href="https://github.com/opencv/opencv_contrib/blob/master/README.md">https://github.com/opencv/opencv_contrib/blob/master/README.md</a> </p><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>主要有两种方法：一种是使用CMake-gui这个图形化界面进行安装（适合对Ubuntu的命令不太熟悉的同学）；另一种是用终端直接输入命令操作，可以参考其他博客，这里就不再讨论。</p><p>顺带讲下环境配置的问题：看教程是一部分，配置多了也要有自己的思考，比如：为什么要这么做？环境变量的作用是什么？这样才能在之后各种环境配置中更加流畅。</p><h3 id="关于版本问题"><a href="#关于版本问题" class="headerlink" title="关于版本问题"></a><strong>关于版本问题</strong></h3><p>虽然目前已经更新到Opencv4，但我们依然可以用Opencv3，为什么？因为RM比赛一定要考虑做出来机器的稳定性，老版本可能不是最先进的，但比较之下是更稳定、更成熟的，如果说创新决定了一个团队的上限，那么稳定性则决定了一个团队的下限。</p><h2 id="安装CLion和PyCharm"><a href="#安装CLion和PyCharm" class="headerlink" title="安装CLion和PyCharm"></a>安装CLion和PyCharm</h2><p>CLion编写C++，PyCharm编写python(神经网络会用到)，python 需要注意配置环境，个人觉得不需要安装Anaconda，直接安装Python&gt;3.5，然后用pip配置环境(tensorflow或pytroch和opencv-py….)</p><p>软件安装不需要去找破解，直接注册学校的教育邮箱 <a href="https://blog.csdn.net/qq_40784315/article/details/116106252">https://blog.csdn.net/qq_40784315/article/details/116106252</a> ，然后绑定自己的 Jetbrains 账户即可免费使用。</p><h2 id="学会使用cmake"><a href="#学会使用cmake" class="headerlink" title="学会使用cmake"></a>学会使用cmake</h2><p>不要求cmake使用的多么6，但至少可以做到：不用CLion也可以编译简单C++程序，会编写CLion中的CMakeLists.txt文件。</p><img src="https://s2.loli.net/2022/02/26/v7zkNnK6uqVPWgd.jpg" alt="" style="zoom:25%;" /><h2 id="工业相机的选择"><a href="#工业相机的选择" class="headerlink" title="工业相机的选择"></a>工业相机的选择</h2><p>针对运动机器人和灯条识别，我们比赛用的工业相机一般选择较高帧率较低像素的配置。镜头焦距一般选用6或8mm。本届相机我们用的是Mindvision的，分辨率和帧率分别为1920*1200和165FPS，针对联盟赛，镜头焦距选的6mm（我们这批发过来的镜头尺寸明显比之前8mm大很多，但是老师那边代购说根据预算只能提供这款，下届视觉组可以提前写明需求买小尺寸的镜头，体积小重量轻显然对结构设计更有利）。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>若以后战队发展壮大，团队成员之间可能需要进行大量的文件传输，可以直接使用邮箱实现不同系统之间的传输，同时不用安装其他应用，不用考虑人在不在工作室，不用担心文件丢失。<br>学会使用各种学习资源：<a href="https://www.baidu.com/">Baidu</a>    <a href="https://www.csdn.net/">CSDN</a>    <a href="https://www.bilibili.com/">Bilibili</a>    <a href="https://github.com/">Github</a><br>视觉组同学如果有梯子，完全可以在github上进行代码迭代管理，进一步提升团队协作效率。</p><p>————————————————————-先说这么多，之后想到再补————————————————————- </p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉研究生建议</title>
      <link href="/archives/36880.html"/>
      <url>/archives/36880.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1356230976&auto=1&height=66"></iframe></div><p>细想了下，我是接触RM比赛以及计算机视觉后，才有了写博客的想法，所以打算先写点视觉相关的东西。在知乎看到几个回答还不错，这里记录一下，留作后面不断查看回顾。</p><h2 id="如何做好科研"><a href="#如何做好科研" class="headerlink" title="如何做好科研"></a>如何做好科研</h2><p>（1）无他，唯手熟尔，就是多读多写。论文就是越读越快的，我还是本科生的时候我导师告诉我刚入门时候要精读十篇好文章，我现在指导师弟师妹也是给他们挑好几篇文章让他们反复精读，起初水平不够的时候看论文很慢，经常抓不住重点都很正常不用急躁。读多了自然就越来越快了，后期看个摘要，看个插图甚至看个题目都明白大致内容了。关于coding就是多写，多去模仿，非计算机科班出身的我刚读博的时候代码很差，现在也不算好，但是这几年在学习别人的代码后也在慢慢提高，所以现在对于刚进实验室的师弟师妹我都对他们的代码有点要求，至少要做到模块化和复用性，现在github开源这么多有很多好的代码可以去模仿学习，跑跑开源代码多去写写，非常忌惮一个现象就是不动手去写，总是只跑开源代码，跑一个换一个框架，没有开源就什么也不会了，要动手自己去复现一些程序。</p><p>（2）good idea，这个我一直认可我导师的观点。我导师说有时候创新点不是整天天马行空想出来的，而是在你看了很多文章做了很多实验之后自然而然产生的，我的感觉也是这样，当你看了很多文章之后，你会发现解决A问题的思路可能在B问题上能够得到启发，当你做了很多实验之后你才能发现论文里隐藏的瑕疵点或者在某些场景下不适用，发现了问题很自然的就能想到一些解决的ideas。</p><p>（3）现在做视觉的人太多，全世界最聪明的一批人都在做cv dl ai，所以发表一篇好paper真的不简单，你的idea别的学术老油条们很容易就能想到，做的还比你漂亮，所以横向比较那些做理论方面的博士同学可能成果比你早比你多，一定要沉得下心保持自信持续努力，厚积而薄发。</p><p>（4）博士我认为还是要open一点，闭门造车一个人搞还是太难，多去接触接触大牛们的工作，多去听听别人的分享，并且也勇于分享自己的见解与工作，多与别人交流讨论，是有帮助的，多于一个博士而言，除了做出好的工作以外，writing 和presentation 的功力也非常重要，如何精确简洁的用文字描述你的工作，如何面对不同类型的听众来做一个合适的报告，都是很重要的技能，这些平时多思考，多学习，多尝试都能慢慢提高。</p><p>（5）努力与专注，做科研并不有趣，需要你自己内心对科研真心的喜欢，会自主的去关注最新的工作，会为自己做出的工作有自豪感，能够从研究工作找到乐趣，保持每天都努力，完成这场博士学位的马拉松。</p><p>（6）对于视觉而言，arxiv的cvpr版块是每天都必须关注的，cvpr iccv eccv三大顶会，pami ijcv等顶刊的论文必须熟知，iclr npis aaai等次相关的顶会也要关注。tf pytorch keras mxnet caffe等框架至少熟用一个，也要关注这些框架的更新动态。新智元，机器之心，paperweekly，ai研习社等等工作号也都可以关注，每天都会发布领域最新动态。旷视科技，商汤，微软亚研院，图森，依图，腾讯优图等等企业也都可以去申请实习，里面真的大牛太多，资源太好，会学到很多东西。</p><p>（7）愿你做有用的科研，做最好的工作。这里我非诚喜欢胡国圣老师在朋友圈发的三句话，希望他不要打我——喝最烈的威士忌,日最野的哈士奇,做最难的Research。</p><h2 id="微软亚洲研究院的回答"><a href="#微软亚洲研究院的回答" class="headerlink" title="微软亚洲研究院的回答"></a>微软亚洲研究院的回答</h2><p>作者：微软亚洲研究院<br>链接：<a href="https://www.zhihu.com/question/67257036/answer/487324016">https://www.zhihu.com/question/67257036/answer/487324016</a></p><p><strong>针对这个问题，在计算及视觉领域拥有17年的研究经历的微软首席研究员华刚博士有些话想和大家分享，他写下了这篇文章，希望能对计算机视觉领域内的学生和年轻的学者们有所帮助。</strong></p><p>简单说一下这篇文章的背景：从我2015年回到微软亚洲研究院之后接触到很多聪明的实习生。一方面感受到他们对计算机视觉研究的热情，另方面也有感于他们对计算机视觉研究认知的局限性，或者说大一点，是基本研究方法和思路上的局限性，就有想法要对如何做好计算机视觉的研究写点什么。计算机视觉领域国际权威、加州大学洛杉矶分校的朱松纯老师曾经发表一篇关于计算视觉的三个起源和人工智能的评论，全面深刻，引起了很大的反响。我想结合朱老师评论的内容，和我在计算及视觉领域17年的研究经历，也来谈谈如何做好计算机视觉的研究，希望对领域内的学生和年青的研究员能有所帮助。</p><p>“如何做好计算机视觉的研究？”要回答这个问题，我们先要对这个问题的关键词进行分析：如果去掉“计算机视觉”这个限定词，这问题就变成了“如何做好研究？”那么，要回答这个问题，我们就要知道“什么是好的研究？”</p><p>而要定义什么是好的研究，必须回到根本，先要知道：</p><h3 id="什么是研究？"><a href="#什么是研究？" class="headerlink" title="什么是研究？"></a>什么是研究？</h3><p>我们的讨论就从这个问题开始。什么是研究？一个被普遍接受的对研究的广义定义为：研究是为了产生新的知识或者是为已有的知识设计新的应用的系统性的工作。因为我们今天的讨论其实更多集中在科学研究上，先确定狭义的研究的定义为：利用科学的方法来调查解释一个现象或者获取新的知识。</p><p>综合这两个定义，可以看到科学研究从本质上是由三个基本的要素构成：1) 目的：产生新的知识或者是设计出新的应用; 2）手段：科学的方法。缺少这两个要素任何之一都不构成科学研究; 3) 成果：新的知识。所谓新的知识，必须是前人不知道的东西。</p><p>我们很多同学和年轻的研究员认为研究就是写论文、研究成果就是论文，这其实是在观念上走进了一个误区。论文是系统阐述新的知识、新的应用，以及阐述获取这个新知识或者新应用用到了什么样的科学方法的一个载体。论文，作为阐述研究成果的主要手段，必须经过同行的评议通过才能正式发表和被认可。</p><p>在人工智能进入第三个热潮之际，我们看到各种各样关于AI的各种媒体报道层出不穷，一方面，这对大众普及了AI各方面的知识，是积极的。但从另一个方面讲，很多观点没有经过仔细的推敲，也没有同行的评议，一些谬误或者是夸大的观点可能因为广泛传播而被大众接受，结果产生负面的社会影响。这就提醒我们相关领域的研究人员，在对大众媒体去做一些评论的时候，必须仔细斟酌，尽量不传播没有得到检验的观点。</p><p>这就谈到第二个问题：</p><h3 id="什么是好的研究？"><a href="#什么是好的研究？" class="headerlink" title="什么是好的研究？"></a>什么是好的研究？</h3><p>不同领域的研究员对这个问题可能会有不同的看法。</p><p>从计算机科学的角度来讲，尤其是计算机视觉的研究，无论是理论的还是实践的，我们的研究成果最终是要解决现实世界的问题的。在这个方面，我印象比较深刻的还是我在西安交通大学读研究生的时候，沈向洋博士2001年在西安交大做报告提到的一个观点：最好的研究员发现新问题；好的研究员创造新方法解好问题；一般的研究员跟随别人的方法解问题——大家在多次这里看到“新”这个关键词，创新是研究的本质。</p><p>有了这些铺垫，我们首先定义什么是最好的研究。通常认为一个领域中对于某一个问题最好的研究工作有三种：第一篇论文 (The First Paper)，最好的一篇论文 (The Best Paper)，以及最后一篇论文(The Last Paper)。这第一篇论文的含义是说这篇论文率先提出了一个好的问题和方向。最好的一篇论文是什么？那一定是开创性地提出了一种解法，启发了最终解决这个问题的途径。至于最后一篇论文，那一定是彻底把这个问题解决了，从此以后这个问题不再需要继续做进一步的研究。从计算机视觉领域举一个具体的例子来讲，Harris Corner Detector属于最早的一批在图像中检测角点的论文，可以归为第一篇之列。David Lowe博士的SIFT特征检测和局部描述子，可以归为在这个方向上最好的论文之列。那么这个方向的最后一篇呢？ 我认为可能还没有出现。具体到我自己的研究工作，在局部描述子这个方向上，我跟我的同事Matthew Brown和Simon Winder在2007年到2009年之间所做的一系列用机器学习的方法来建立描述子的工作，也实际上为提高局部描述子的性能提供了一个新的思路和方法。</p><p>对于我们很多研究员和学生来讲，一辈子可能都做不到这三种最好的研究工作之一。那是不是就等于说你不能做好的研究工作或者根本不用考虑做研究了呢？肯定不是这样。科学研究是一个共同体。这些最好的研究工作也是在前面很多很多非常扎实（solid）的研究工作的基础上发展出来的。因此，对于年青的研究员和学生而言，应该胸怀大志，去追求做最好的研究工作，但从实际执行上来讲，还是要把一项一项具体的工作先做扎实了。</p><p>怎么做到把研究工作做扎实了？首先，你必须对你要解的问题有一个全面深刻的了解，包括为什么要解这个问题、解这个问题有什么意义呢、以前有没有试图解决同样或者类似问题的先例，如果有，你就要全面了解前人都提出了什么样的解法、他们的解法都有什么样的优势和缺陷……最后，你的解法解决了前面这些解法不能解决的问题呢，或者是你的解法处理了什么样的他们不能处理的缺陷了？这些问题的答案如果都有了，那么，在写论文的过程中要注意的就是，1）你的假设是什么？2）你怎么验证了你的假设？这个验证既可以是理论上的证明，也可以是实验的验证。我们很多学生和年青的研究员，写论文的时候没有找到内在的逻辑关系，很多观点都是似是而非。或者说重一点，在论文撰写方面的训练严重不足。你的研究如果到了写论文的阶段，那就必须要有明确的观点提出来。这个观点必须明确无误，只有这样你才能被称为形成了新的知识。你的每一个观点都必须在理论上或者是实验中得到验证。另外，论文的撰写是为了让人看懂，不是让人看不懂，所以我们在撰写过程中必须尽量保证不去假设读者已经拥有了某些方面的知识。做好了这些，基本上你就有很大的可能性能够做出扎实（solid)的研究工作。</p><p>然后回到我们讨论的主题：</p><h3 id="如何做好计算机视觉的研究工作？"><a href="#如何做好计算机视觉的研究工作？" class="headerlink" title="如何做好计算机视觉的研究工作？"></a>如何做好计算机视觉的研究工作？</h3><p>其实，要回答这个问题，将我上面讲的所有观点加上“计算机视觉领域”这个限定词就行了。我这儿结合计算机视觉研究的一些现状及朱松纯老师的一些观点来进一步谈谈我的观点。</p><p>首先谈谈我观察到的一些现象。很多年轻的学生，现在讨论问题的时候都用这样的谈话：我发现用FC6层的特征，比用FC7层的特征，在某个图像数据集上比现在最好的算法提高了1.5%的识别精度，老师我们可以写论文了（如果大家不能理解这句话，FC6和FC7是表示AlexNet的两个中间输出层）。我想请问，你在这个过程中发现了什么样的普适的新的知识吗，又或者，在不是普适的情况下，你在什么限定条件下一定能够看到这样的识别精度提高了？</p><p>不错，提高识别精度是一个很好的目标，但要注意，计算机视觉的研究是要解决识别的问题，不是解某一个图像数据集。这些图像数据集提供了很好的验证你的假设和方法的手段，但如果你没有遵循科学的方法和和手段去设计你的算法和实验，你也不可能得到一个科学的结论，从而也不能产生新的知识，更不用谈对这个领域做出贡献。朱松纯老师在他的评论中提到，很多学生认为，计算机视觉现在就是调深度神经网络的参数，也就是说的这个问题。</p><p>所以，具体到对于刚开始从事计算机视觉研究的学生来讲，要做好这方面的研究，我觉得第一步还是要系统学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的来龙去脉、这个领域都有哪些基本的问题、哪些问题已经解得比较成熟而哪些问题还在初级阶段……这里，推荐所有的学生学习两本经典教材《Computer Vision: A Modern Approach》和《Computer Vision: Algorithms and Applications》，可以先读完第一本再读第二本。</p><p>只有对这个领域有了一个初步的全面了解，你才能够找到自己感兴趣的那个问题。在众多的问题当中，你是希望做三维重建，还是做图像识别、物体跟踪，又或是做计算摄影呢？做研究其实不是一个完全享乐的的过程，你必须要有足够的兴趣来保证你能持续地走下去，这在你感觉自己当前研究的思路走不下去的时候尤其具有重要意义。当你确定你感兴趣的问题，你应该首先全面调研一下这个问题的来龙去脉。这就意味着你不能只读过去五年的论文。你可以从过去一年的论文开始，慢慢追溯回到过去很久的相关的论文。有些时候，你会惊讶地发现前人想问题的深度。研究的英文单词是Research，拆开是Re-Search，用中文直译就是重新搜索和发现，而不是直接发现，其实就是说你要首先对这个问题做追本溯源。朱松纯老师提到的我们很多学生现在不读五年以前的论文，说的也是这个道理。</p><p>当你做好了这些，你必须钻进计算视觉的一个小的领域。人的精力是有限的，这就意味着你不可能把很多事情同时做好，所以在你选好方向之后，就要把你的精力集中在你感兴趣的一个问题上， 努力成为这个方面的专家。研究是一项长跑，很多时候，你在一个方向上比别人坚持久一点， 你就有机会超越他而成为某个方面的专家。最后，我也来谈谈深度学习对计算机视觉的影响。在这里，我对马里兰大学Rama Chellapa教授在Tom Huang教授80岁生日论坛上表达的观点非常认可，他认为，深度学习网络就像一个Pasta Machine：你把该放的东西放进去，它能给你产生好吃的Pasta。同时它也是一个Equalizer：无论你在计算机视觉领域有40年的经验还是0年的经验，只要你会用Caffee，你在一些问题，比方说图像识别上，都能产生差不多的结果。他开玩笑说这有点伤自尊 (It hurts my ego!)，但我们还是应该把它作为一个好的工具拥抱它。我想，他的言外之意，是我们的研究应该做得更深，要去理解这个工具为什么能够工作得比较好，从而产生新的知识去指导将来的研究和应用。</p><p>我认为，对于年轻的学生来讲，从深度学习的方法开始学习没有什么问题，但必须要进一步去了解一下其他的数学和算法工具，像统计贝叶斯的方法、优化的方法、信号处理的方法等等等的。计算机视觉的问题，其本质是不适定的反问题，解这一类问题需要多种方法的结合。这里面有深度学习解得比较好的问题，像图像识别，也有深度学习解不了的问题，像三维重建和识别。</p><p>任何研究领域包括计算机视觉的研究，对处在研究初期的学生而言， 更重要的是掌握足够的数学工具，培养一种正式思维（Formal Thinking）的能力，这样，遇到实际的问题就能以一种理论上正确的思路去解决这个问题。</p><p>作为结束语，我想对在从事或者有志于从事计算机视觉研究的学生说，计算机视觉的研究处在一个非常好的时期，有很多我们原来解不了的问题现在能够解得比较好了，像人脸识别，尽管我们其实还没有从真正意义上达到人类视觉系统对人脸识别的鲁棒程度。但我们离真正让计算机能够像人看和感知这个世界还有很远的距离。在我们达到这个目标之前，深度学习的方法可能是这个过程中一个重要的垫脚石，同时我们还要将更多的新的方法和工具带入这个领域来进一步推动这个领域的发展。</p><h3 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h3><p>华刚博士是微软首席研究员，研究重点是计算机视觉、模式识别、机器学习、人工智能和机器人，以及相关技术在云和移动智能领域的创新应用。他因在图像和视频中无限制环境人脸识别研究做出的突出贡献，于2015年被国际模式识别联合会（International Association on Pattern Recognition）授予”生物特征识别杰出青年研究员”奖励，因其在计算机视觉和多媒体研究方面的杰出贡献，于2016年被遴选为国际模式识别联合会院士（IAPR Fellow）和国际计算机联合会杰出科学家(ACM Distinguished Scientist) 。他担任过CVPR 2014/2017、ICCV 2011、ACM MM 2011/ 2012/ 2015/2017、ICIP 2012/2013/2015/2016、ICASSP 2012/ 2013等十多个顶级国际会议的领域主席，将担任CVPR 2019的程序主席。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 科研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/archives/16107.html"/>
      <url>/archives/16107.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=16139397&auto=1&height=66"></iframe></div><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
