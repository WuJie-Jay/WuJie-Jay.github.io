<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>小论文投稿（试水）</title>
      <link href="/archives/375e7a9.html"/>
      <url>/archives/375e7a9.html</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1974910967&auto=1&height=66"></iframe></div><p>前不久还在考虑开题报告的事情，本周看到导师发的学校对硕士研究生毕业成果要求，其中有一条是在统计源期刊（中国科技核心期刊）公开发表或正式录用学术论文1篇。想到手里正好有一篇竞赛获奖论文，于是了解了下有关期刊投稿方面的内容，开始着手准备。</p><h2 id="国内七大核心期刊体系"><a href="#国内七大核心期刊体系" class="headerlink" title="国内七大核心期刊体系"></a>国内七大核心期刊体系</h2><p>1.北京大学图书馆俗称“北大核心”；</p><p>2.南京大学“中文社会科学引文索引（CSSCI）”；</p><p>3.中国科学技术信息研究所“中国科技论文统计源期刊（CSTPCD）”；</p><p>4.中国社会科学院文献信息中心“中国人文社会科学核心期刊（CHSSCD）”；</p><p>5.中国科学院文献情报中心“中国科学引文数据库（CSCD）”；</p><p>6.武汉大学“中国核心期刊目录（RCCSE）”；</p><p>7.CNKI“中国引文数据库（CCD）”。</p><p>以下是各类期刊体系概况图：</p><p><img src="https://s2.loli.net/2022/09/18/C7hFaHwALoQVKJU.jpg"></p><p>目前世界上的论文权威性等级大致是这样划分的：</p><p>SCI源刊（SSCI源刊）＞EI源刊＞中文核心（南大核心-CSSCI、北大核心等）＞EI会议、CPCI会议＞国际级、国家级期刊＞省级期刊＞其他普刊。</p><p>在基础学科领域，SCI期刊在国内的认可度较高，而在工程技术领域EI认可度相对较高。相对中文核心期刊，SCI、EI国际认可度更高一些。</p><h2 id="中国科技论文统计源期刊"><a href="#中国科技论文统计源期刊" class="headerlink" title="中国科技论文统计源期刊"></a>中国科技论文统计源期刊</h2><p>中国科技论文统计源期刊”（亦称中国科技核心期刊，The key magazine of China technology），是中国科学技术信息研究所出版的中国科技论文统计源期刊。学科范畴主要为自然科学领域，是国内比较公认的科技统计源期刊目录。受科技部委托，权威性名列国内首位。期刊目录会出现在中国科技信息研究所每年公布一次的 《中国科技期刊引证报告》中。中国科技信息研究所 (ISTIC)是受国家科技部委托，每年进行遴选和调整，权威性名列国内首位。</p><p>从1987年开始对我国科技人员在国内外表论文数量和被引1用情况进行统计分析，并利用统计数据建立了中国科技论文与引文数据库（CSTPCD），受到社会各界的普遍重视和广泛好评。通过中国科技期刊综合指标评价体系对期刊学术质量的考核，CSTPCD每年对收录期刊的范围进行调整。中国科技论文与引文数据库(CSTPCD）是理科方向的检索工具，比较适合理工科专业科研人员。</p><p>这里附上2022年最新科技核心期刊目录链接（2021年底评选的科技核心，有效期为2022年全年）：<a href="http://lib.cpu.edu.cn/_upload/article/files/95/12/1271ec824d10a3a9f1ce19ce5ee6/d0625174-0f93-4657-8061-a047416a8ade.pdf">d0625174-0f93-4657-8061-a047416a8ade.pdf (cpu.edu.cn)</a></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>由于我的论文是图像分析处理，有关深度学习、神经网络的内容，所以选择计算机方向期刊投稿通过率更高，同时选择也因为是首次投稿，保险起见还是选择了相对靠后的一些核心期刊，虽然这样影响因子比较低，但相对更容易通过，较为合适。本周工作主要还是根据预选的几个期刊网站要求修改小论文，同时电话、邮件联系相关编辑部，针对投稿问题进行咨询，后期有问题会继续跟进。</p><p>最后，汇总一些投稿过程中需要注意的问题：</p><ol><li>选择期刊要考虑周期，属于哪类期刊，是什么级别，征稿要求是什么，发表周期大致多长时间。</li><li>搞清单位或地区评价标准。比如有些地方对期刊有评职方面明确的要求，在发表时不先确认好这些信息，导致发表的刊物不能用，只能哑巴吃黄连。</li><li>着急见刊就匆匆忙忙找人帮忙发表文章。这是一个普遍的现象，也是出现问题最多的一点，很多作者在评职称之前的两三个月才着急发表论文，这时候符合要求的刊物已经很少了，只能死马当活马医，许多作者因此吃大亏。</li><li>一味图价格便宜。俗话说一分钱一分货，对于期刊的发表费用来讲，一些作者图便宜，结果论文安排并不顺利，包括不能按时见刊，发表之后不能用，甚至是根本就发表不了。要多了解几个，自己首先心里有底，怎样的期刊，目前发表的普遍价格是多少，清楚后才可以谈。</li><li>对刊物信息不加核实。即使是论文服务机构推荐的期刊，自己也要进行必要的核实，新闻出版总署、知网、万方等数据库都查一查。</li><li>审稿通过后不及时支付版面费。很多作者认为审稿通过了，编辑部不会轻易撤稿，这种想法是错误的，审稿通过之后，缴纳了版面费用才会安排刊期。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科研论文 </tag>
            
            <tag> 视觉 </tag>
            
            <tag> 投稿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开题相关心得</title>
      <link href="/archives/aa0d1297.html"/>
      <url>/archives/aa0d1297.html</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1374329431&auto=1&height=66"></iframe></div><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>针对“基于机器视觉的运动目标特性分析及识别方法研究”这一题目，在知网、百度学术、Sci-Hub等网络知识平台，以及部分专利网站搜集了相关方向的文献资料后，对其进行了归纳总结，以期为后续论文开题及课题研究作铺垫。</p><p>通过分析论文摘要、关键词，快速了解全文的研究重心和工作方法。此外，对硕士、博士、期刊、专利等不同类别的文章作了简要的分析对比，对相关研究方向和领域也有了新的认识。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="涉及知识点概要"><a href="#涉及知识点概要" class="headerlink" title="涉及知识点概要"></a>涉及知识点概要</h3><p>运动目标检测与跟踪是指在目标的运动信息未知的条件下，通过光学传感器采集的图像信息，分析出该目标的相关信息。<strong>目标检测</strong>是通过相关算法，得到有用的运动目标信息，是目标跟踪实现的基础。<strong>目标跟踪</strong>是在该基础上，在帧与帧之间建立运动目标的某些特征如位置、速度、纹理、颜色、形状等之间的联系，分析其运动状态。运动目标的检测算法主要是针对背景数据的建模与模型更新，而跟踪算法主要是针对目标数据的建模与模型更新。</p><p><strong>运动目标分类方法</strong>主要分为三类：<strong>基于目标静态特征的分类</strong>、<strong>基于目标运动特性的分类</strong>和两种方法的结合。在现今的研究中还不存在通用检测方法，即还没有一种针对不同环境都能检测出完整动态目标区域的检测方法。并且当运动目标发生遮挡或重叠时也不能提取到独立完整的目标区域。一个成功的目标分类方法应该同时具备实用性、实时性以及鲁棒性。运用机器学习进行图像中的目标识别与分类主要有两个核心点：<strong>目标特征的提取</strong>和<strong>分类器的训练</strong>，选择合适的特征对于目标识别的准确率至关重要。</p><ul><li>常应用到的图像特征包括颜色特征、纹理特征和形状特征等。颜色特征是一种应用广泛的视觉特征，其对于图像本身的尺寸，方向和视角的依赖性小，特征提取容易，常用的表达颜色特征的方法有颜色直方图，颜色集等；</li><li>纹理特征刻画了像素的领域灰度空间分布规律，根据目标物体纹理信息差异进行识别分类，表达纹理特征的方法有 LBP 特征，Gabor 特征等；</li><li>形状特征是指目标物体的边缘形状，长宽比，面积大小等几何特征，根据形状的不同来进行目标的分类，表达形状特征的有 Haar 特征，SIFT 特征，HOG 特征等。</li></ul><p><strong>图像处理基础知识</strong>：灰度化、二值化、滤波、形态学处理、压缩、分割等等，常借助opencv开源库进行处理。</p><p><strong>运动目标检测主要方法</strong>：<strong>光流法</strong>、<strong>帧差法</strong>、<strong>背景差分法</strong>。</p><p>在常见的基于视频序列运动目标检测方法中，帧差法是取前一帧序列图像将其作为参考图像，然后后一帧与其进行差分计算并阈值化得到运动区域。它的特点是时间复杂度较低并且运算比较简单，是最简单的运动目标检测方法，在实时检测的系统中运用广泛，但也具有一定局限性，对于速度缓慢的运动物体很容易检测出空洞，并且需要保证帧间的背景是静止一致的。鉴于帧差的不足，有许多学者在帧差法的基础上提出了多帧差法比如三帧差、四帧差还有取不相邻的帧做帧差法等。</p><p>光流法通过计算速度场来区分运动目标和背景。其特点是既能适用于静态场景也能适用于场景变化的场景。由于计算量太大而造成实时性差使其无法应用到实际场景中。</p><p>背景差分法中最关键步骤是建立背景模型并根据当前帧信息实时更新。它的特点是可以提取较为完整的运动目标，但是该方法对场景变化十分敏感，因此当场景中出现光照、水波反射等变化很容易在检测效果图中出现伪运动点。从整体上来说该类方法能够得到比较完整的运动目标信息并且思路清晰操作简单，因此在当前的研究中都得到广泛的应用。其中，高斯混合模型是一种多背景模型的背景差分方法，具有较好的适应性并且能较好地建立和更新背景模型 。</p><p><strong>可用的数据集</strong>有：行人数据集INRIA Person Dataset，机器视觉汽车图像检测数据集Computer vision opencv car dataset，大型物体检测数据集COCO，自动驾驶数据集Cityscapes，具有目标类别语义标签的视频集CamVid等。</p><h3 id="硕士论文"><a href="#硕士论文" class="headerlink" title="硕士论文"></a>硕士论文</h3><p>在近十年的硕士论文中，简单场景下，对于运动目标检测，一般利用改进<strong>Vibe</strong>（Visual Background Extractor，视觉背景提取）算法，可有效地消除鬼影和阴影现象；利用改进KCF（Kernelized Correlation Filter，核相关滤波）算法和<strong>SiamRPN</strong> 算法，可对模型自适应更新，提高精度及综合性能；使用 <strong>PCA</strong>（Principal Components Analysis）算法对特征向量进行降维以缩短检测时长。</p><p>对于识别和分类，一般借助<strong>SVM</strong>（Support Vector Machine，支持向量机）分类器对<strong>HOG</strong>（Histogram of Oriented Gradient，方向梯度直方图）特征训练，对人和车辆快速识别。采用<strong>Sage-Husa</strong>（自适应卡尔曼滤波）进行改进，提出防止滤波发散的方法，减少估计误差，提高估计准确性和实时跟踪性能 。或是先利用ACF （Aggregated Channels Feature）算法检测图像中的行人和车辆，然后利用基于CNN（卷积神经网络）模型对目标再次识别筛选提高图像目标检测的准确率，并结合<strong>MSER</strong>算法提取运动目标区域。</p><p>在复杂动态场景下，对目标的识别还会存在光照变化、姿态变化、遮挡等问题，计算量大，一般利用基于帧差分块和自适应学习率的<strong>GMM</strong>（混合高斯模型）改进算法、基于自适应子空间学习的粒子滤波跟踪算法，结合 <strong>Mean Shift</strong> （均值偏移）聚类算法，使其提高检测灵敏度，具有适应目标外观快速变化的能力。</p><h3 id="博士论文"><a href="#博士论文" class="headerlink" title="博士论文"></a>博士论文</h3><p>在最新的一些博士论文中，还提出了：</p><ul><li>一种结合深层特征与鲁棒特征融合的孪生卷积神经网络目标跟踪算法；</li><li>一种高效的自适应多层特征融合策略；</li><li>一种前景信息引导的孪生卷积神经网络目标跟踪算法；</li><li>一种联合改进局部纹理特征和辅助重定位的生成式跟踪算法；</li><li>一种基于动态空间正则化和目标显著性引导的相关滤波跟踪算法；</li><li>一种多特征耦合和尺度自适应的相关滤波跟踪算法；</li></ul><p>……</p><p>相比于硕士学位论文，博士论文更加偏向于理论方向的研究，相关知识体系更加深入，研究内容也更加具体。另外还有对<strong>运动人体</strong>进行<strong>特征分析</strong>的算法，包含了对外形、肤色、姿态等不同特征的匹配。同时也注意到，近几年的硕士论文与早期的博士论文研究内容有许多共通之处，侧面反映出如今硕士毕业生对专业认知有了更高要求。</p><p>此外，将<strong>机器视觉</strong>与<strong>毫米波雷达</strong>结合来进行对车辆与行人的检测，可有效提高行人检测的准确性和可靠性。对于多传感器的<strong>信息融合</strong>，由于各传感器返回的数据表征目标不同的特征信息，因此需要对多源信息进行多层次多步骤的分析融合，根据融合时传感器数据的抽象程度，可将融合层次划分为三个等级：<strong>数据级</strong>、<strong>特征级</strong>以及<strong>决策级</strong>。</p><ul><li>数据层的融合就是直接将各传感器返回的数据融合，可以保留原始数据的所有目标信息，避免信息损失，但因此也会造成信息处理和通信量巨大、分析时间长、实时性差等问题。</li><li>决策级的信息融合是将各传感器返回的目标数据进行预处理以及特征提取，得到各自的决策结果，再将结果进行整体的分析融合。在该层次进行信息融合时信息量和数据传输量都较小，但是存在较高的处理代价。</li><li>特征层的融合需要对各传感器的原始信息进行数据处理，提取出特征信息，再进行特征级的融合。该层次的融合兼顾了信息损失和抗干扰能力等方面，对以上两种层次的融合进行了优势互补，融合效果较好。</li></ul><hr><h3 id="期刊论文及专利"><a href="#期刊论文及专利" class="headerlink" title="期刊论文及专利"></a><strong>期刊论文及专利</strong></h3><p>期刊论文和专利文章一般篇幅有限，且期刊为了节省版面，大多采用双栏排版。关于运动场景目标识别，期刊文章多数是研究某一点，根据现有的工具和算法，提出一种或两种改进优化方法，其中有几篇内容比较新颖，如“针对雨雪天气条件下的运动目标检测受到天气的影响较大，提出一种融合全变分正则化和约束鲁棒主成分分析模型的视频序列运动目标检测算法”。此外，更多文章写的是当前领域研究综述，其中大多是结合市场热门行业应用进行分析。期刊文章的质量参差不齐，需要自行辨别，一般而言，<strong>核心期刊</strong>的文章相对而言可读性更高，这一点从本次搜集查找的论文资料可以比较直观地看出来。</p><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>根据本周阅读的文献资料来看，目前大部分“运动目标特性分析及识别方法研究”都是基于图像本身进行分析，少有从<strong>运动学</strong>和<strong>动力学</strong>方面考虑的文章，这方面是下一阶段的学习重点，我会在继续研究该分类文献的同时，拓展自己的知识面，希望能将两部分联系起来，产生一些新的思路。</p><p><img src="https://s2.loli.net/2022/09/13/b18DWeCyqSXKOTz.png"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科研论文 </tag>
            
            <tag> 视觉 </tag>
            
            <tag> 开题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器人热点内容概要</title>
      <link href="/archives/c336656a.html"/>
      <url>/archives/c336656a.html</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=38018486&auto=1&height=66"></iframe></div><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><h3 id="机器人领域的研究方向总览"><a href="#机器人领域的研究方向总览" class="headerlink" title="机器人领域的研究方向总览"></a>机器人领域的研究方向总览</h3><p><strong>1.传感器技术</strong></p><ul><li>基于多传感器信息的机器人路径规划</li><li>多传感器信息融合及其在机器人中的应用</li><li>基于传感器信息的移动机器人精确定位研究</li><li>移动机器人系统中传感器系统的采集及处理</li></ul><p><strong>2.控制系统及其控制算法</strong></p><ul><li>基于PID控制的机器人轨迹跟踪性能研究与比较</li><li>移动机器人的控制方法研究</li><li>轮式移动操作机器人的鲁棒跟踪控制器设计及研究</li><li>开放式机器人控制器的研究</li><li>智能移动机器人的智能控制</li><li>移动机器人神经网络控制研究</li><li>移动机器人模糊控制研究</li><li>移动机器人系统中嵌入式控制器研究</li></ul><p><strong>3.视频处理及视觉伺服控制</strong></p><ul><li>基于DSP的机器人视觉信息处理系统</li><li>主动视觉及其在机器人中的应用</li><li>机器人视觉伺服控制系统研究</li><li>图象特征提取技术研究</li><li>人脸识别技术及其在移动机器人中的应用</li><li>基于光流技术的移动机器人导航系统研究</li><li>精细视频压缩编码及其在移动机器人系统中的应用</li><li>视频采集系统研究</li><li>足球机器人视觉图象识别系统研究</li><li>视频的压缩编码及其在机器人系统中的应用</li><li>小波方法在移动机器人系统中的应用</li></ul><p><strong>4.网络机器人技术</strong></p><ul><li>视频网络传输及其在移动机器人系统中的应用</li><li>基于Agent的遥操作机器人控制器研究</li><li>基于网络的移动机器人控制系统研究</li><li>基于网络的移动机器人直接控制系统研究</li><li>监督式网络控制结构及其在移动机器人中的应用</li><li>移动机器人中视觉临场感遥控系统的研究</li><li>移动机器人分布式控制系统研究</li></ul><p><strong>5.人机交互</strong></p><ul><li>语音识别技术及其在移动机器人系统中应用</li><li>手势识别及其在移动机器人人系统中的应用</li><li>多模态人机交互及其在移动机器人系统中</li><li>虚拟现实技术在移动机器人系统中的应用</li></ul><p><strong>6.机器学习</strong></p><ul><li>移动机器人的再励学习研究</li><li>基于视觉的移动机器人学习系统研究</li><li>基于支持向量机的移动机器人学习</li><li>动态环境下移动机器人的路径规划方法</li><li>数据挖掘技术及其在移动机器人系统中的应用</li></ul><p><strong>7.通讯技术及多机器人协调</strong></p><ul><li>多移动机器人系统合作与协调</li><li>多机器人系统中硬实时通信的研究</li><li>多机器人系统中实时通信研究</li><li>多机器人任意队形分布式控制研究</li><li>蓝牙技术及其在多移动机器人系统中的应用</li></ul><p><strong>8.足球机器人</strong></p><ul><li>移动机器人导航系统研究</li><li>足球机器人进攻策略研究</li><li>移动机器人的目标跟踪研究</li></ul><p><strong>9.机器人系统</strong></p><ul><li><p>全自主移动机器人系统研究</p></li><li><p>移动机器人系统的可靠性</p><p><strong>······</strong></p></li></ul><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">社会上一旦有技术上的需求，那么这种需求会比十所大学更能把科学推向前进。——恩格斯</span><br></pre></td></tr></table></figure><p>截止至2022年5月24日，2022年ICRA总共收到了3344篇文章的申请，最终1498篇文章通过审核上线发表。在这优秀的1498篇文章中，SLAM、Sensor-Fusion和Localization领域的文章简单整理下来有140篇左右，再进一步细分Visual SLAM相关领域的有大约58篇，利用视觉传感器进行定位的相关论文大约有22篇，Lidar SLAM相关领域文章大约是23篇，基于雷达传感器的定位模块的相关论文就只有9篇左右。</p><p><img src="https://s2.loli.net/2022/09/03/7H3cS1lZDPRfY59.jpg"></p><p>由上可见，视觉方向的相关研究相较于激光方向还是更加受科研人员和审稿者青睐，从工业落地和传感器性能发展的角度上分析的话，人们还是希望能将实现该技术所产生的费用降到<em>global minimum</em>（全局最优），同时从发表的论文主题上看，越来越多的优秀科研人员已经在视觉和激光传感器的融合方向上做出了突出的成绩，往后展望多传感器融合的工作还能继续挖掘，并且相对来说较容易做出“成果”，而其他在基本算法（点云注册、配准等）上优化、改进和新理论的工作，虽然在总量上并不突出，但其意义就如同人体的骨架一般，支撑着该领域的持续生长。</p><p>附：2020年ICRA各session的论文数量统计表，文章数量最多的几个session分别为</p><p><img src="https://s2.loli.net/2022/09/03/VqvcNu8LdBx6bwo.png"></p><p>从以上关键词的统计可以看出，动作和路径规划，深度学习， SLAM，多机器人协作系统，移动机器人等都是最近几年比较火的方向。</p><p>附：2019年ICRA各session的论文数量统计表：<a href="https://public.tableau.com/views/ICRA2019_2/Dashboard1?:embed=y&amp;:display_count=n&amp;:origin=viz_share_link">https://public.tableau.com/views/ICRA2019_2/Dashboard1?:embed=y&amp;:display_count=n&amp;:origin=viz_share_link</a></p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先，现阶段我们见到的大多数机器人都还达不到完全自主独立运行的地步，更多情况下，还要依赖人为遥控，如波士顿动力机器人的演示视频。</p><p>然后，区分机器人与另外一个概念——人工智能，机器人通常必须具有实体机械结构，其实是被包含在人工智能这个更宽泛的概念之下。现在大家最常见的几种机器人，包括机械臂，轮式移动机器人，足式机器人以及无人机，这些都是目前发展比较成熟的机器人平台，很多其他形式也处于研究阶段，如软体和仿生机器人等。</p><p>机器人学是一门交叉学科，为了设计制造机器人，需要具备多个传统学科的知识，例如： </p><ol><li>机器人的自由度是多少？关节减速器如何设计或选择？(机械设计与制造专业) </li><li>机器人的关节如何驱动？驱动电路如何设计？(电子工程专业) </li><li>如何规划机器人的运动？如何对机器人的运动进行仿真？(计算机及软件专业) </li><li>如何让机器人跟踪我们规划好的运动轨迹？(自动控制专业) </li><li>如何建立机器人的运动或受力模型？(力学专业) </li><li>如何判断机器人所受约束的类型？(数学专业) </li></ol><p>接下来，我需要考虑，研究机器人主要解决什么问题，包括哪些研究目标？</p><p>机器人学是一个庞杂的学科，在大部分教科书中，一般划分为“<strong>建模、感知、规划、控制</strong>”这几部分。在我最初接触机器人学时就十分好奇：为什么要划分成这几部分，各部分间的关系又是什么？在我读研期间，发现很少有人能阐述清楚它们的关系，似乎大家都默认这是顺理成章的。现在从信息的角度俯瞰全局，我已经不再有最初的困惑了。“信息”贯穿了机器人研究的整个过程。为了让机器人有更好的表现，必须搜寻一切能找到的信息，并最大限度的使用它们：<br>　　●　建模能得到机器人的运动变量间的关系（信息），这是为了预测机器人的行为，但更多时候我们想改变它的行为（控制它按照我们的意愿运动）；<br>　　●　感知则是为了得到周围环境的信息；<br>　　●　当然，得到信息不是目的，我们最终还是要用信息来决策，这就是“规划”的任务了。规划时使用信息可以提高效率，比如在图搜索中常用的启发式信息（这个例子展示了使用信息的好处）；<br>　　●　规划出结果后如何让机器人执行呢？这就是控制的任务了。控制更离不开信息，比如机械臂控制中最常用的计算力矩法里的前馈项就是机器人的动力学信息，由于我们掌握的信息不能做到100%准确，所以有偏差，因此还需要结合反馈控制，反馈控制依赖的误差同样是信息。（有个期刊叫《信息与控制》，由沈阳自动化研究所主办。无独有偶，我们国家唯一的机器人学国家重点实验室就设立在沈阳自动化研究所。此外，还有各种“信息与控制学院”、“信息与控制研究所”。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">“信息不但是控制的基础，还是控制的出发点和归宿，贯穿于整个控制过程的始终。”——《信息科学原理》，钟义信</span><br></pre></td></tr></table></figure><p>上面这几个方向也基本对应这工业界中主要的岗位方向，也就是说，通常一份工作主要专注于其中一个方向就已足够，这也说明该领域跨度非常大，是一个十分复杂的学科。机器人学的主要研究内容与上一篇中提到的自动驾驶有很多交叉重合部分，自动驾驶车辆本身也可看作是一个轮式机器人）。</p><p>具体来讲，第一个方向：机器人感知，直观的理解，它发挥的作用就像人的感知器官，它包括了眼睛，耳朵，鼻子这些传感器，也包括处理这些信息的大脑相关的区域。对应到机器人就是，处理激光雷达，摄像头这些传感器实时接收到的数据，再对这些原始的数据进行处理，最后形成一个综合的对周围环境的感知，当然这里面也包含对机器人自身状态的感知。人可能不能非常清晰的感知到自己内部器官的运行情况，但对机器人来说，是可以的，也是必须要时刻监控的数据。其实只需要记住一点——视觉等各种感官反馈都是为了获取信息。</p><p>这里举了一个例子。下面是飞行器探索未知环境的动画，通过机器人的运动和它携带的传感器，经过slam算法，就可以实现像图例这样的自动探索，自动建图。这张形成的地图就是机器人对环境的感知。</p><p><img src="https://s2.loli.net/2022/09/03/p6QSCyPJrBfMmEn.gif"></p><p>所以机器人感知这块目前可以说是包含范围最广，同时也和深度学习这些人工智能技术结合最紧密的领域，相应的它有关的岗位也最多，涉及到的具体技术，包括计算机视觉，传感器融合，同步建图与定位等等。这里需要学习一些基本的算法原理，并掌握在现有的ROS生态下实现这些算法的软件包，这也是现在大多数初级开发岗位需要的知识水平，如果想继续提高，那就需要专注于某个技术去深入钻研了。</p><p>第二个方向是机器人的决策和规划，这是作为一个机器人的中枢模块，从环节上来说，连接感知和运动控制，根据任务，以及感知到的环境信息，做出决策，以及规划出机器人的运动轨迹。这里可以看出在决策和规划里也可以再分出两个层次，分别是决策和规划，这些名词其实都来自行业的惯例，我们可以在企业的招聘的要求里看到相关的字眼。具体来说，举一个例子，在执行一个任务的时候，机器人在A点，决策模块会决定要完成这个任务，机器人需要运动到B点，这个B点的选择过程就是决策，至于规划，就是给定了a和b点，如何获得ab之间的可行的轨迹的这个过程。所以，可以看出，决策过程比规划要更加抽象，而规划则更加具体。大部分时候，这两个模块都是紧密关联的，很多招聘也都是在岗位设置中把决策和规划放到一起。</p><img src="https://s2.loli.net/2022/09/04/5AQB1USR8vzioXP.gif" style="zoom: 67%;" /><p>第三个方向是运动控制，它的任务就是控制机器人从一个状态到另一个状态，比如下面控制机器狗完成跳跃的动作。这个方向是一个比较新的方向，之前大家都在做轮式机器人的时候，这个方向的工作很简单，只需要很简答你的控制技术就可以精确控制轮式小车，现在随着新的机器人平台出现，比如四足机器人，人形机器人，这个方向也变得越来越热门，涉及到的技术包括模型预测控制，强化学习控制，这些就远远超过了这门课程的范畴了。在这门课程后，大家可以做到的是理解运动控制在一个完整的机器人系统和应用中，运动控制发挥的是末端的执行作用，由更上层的逻辑发送命令，运动控制负责底层的执行，具体到驱动关节的电机的控制。</p><p><img src="https://s2.loli.net/2022/09/03/hV4RGZiga5ztLES.gif"></p><hr><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>既然slam方向目前有这么多优秀的工作发表和开源了出来，那么回归一下问题：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Is SLAM sloved？</span><br></pre></td></tr></table></figure><p>按照目前的情形来看，应该还是没有的。为什么大家都这么卷了，却还是没有达到SLOVED的状态呢？</p><p>这里想直接引用高翔博士的一句话：</p><p><strong>“开源的LIO大部分只能在自己的数据集上跑，换一个数据集就很容易飞或者挂。前期的方案考虑的东西太少，在后出的数据集上通常会出问题。近期的方案则相对要稳定一些，但依然没有纯Lidar方案那么稳定，各种指标也有一定的提升空间。”</strong></p><p>从高博的意思，学术届的工作中考虑的并不需要那么全面，而是将某一个方向做精做优，而到了行业内的大规模落地上，工程师们需要考虑的就是整个系统的方方面面了，就算精度达到了亿分之一也不敢直接就说我的产品是Top one了，当所有的模块紧紧耦合在一起的时候，那需要顾及到了复杂度就不单单是和系统模块的比例增长了。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科研论文 </tag>
            
            <tag> 机器人 </tag>
            
            <tag> 研究热点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自动驾驶概要</title>
      <link href="/archives/29d025d8.html"/>
      <url>/archives/29d025d8.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535056564&auto=1&height=66"></iframe></div><h2 id="行业现状"><a href="#行业现状" class="headerlink" title="行业现状"></a>行业现状</h2><p>粗略来说，自动驾驶公司可以分为两大类别：</p><p><strong>一类是传统的车企</strong>（比如国外的大众，宝马，通用，丰田等，国内的长城，吉利等），<strong>新能源车企</strong>（比如特斯拉，蔚来，小鹏等）和<strong>Tier1</strong>（比如国外老牌的博世，大陆，安波福等，以及国内新兴的华为，大疆等）。这类公司的首要目标是量产，一般以L2级别方案为主，目前也在向L3级别扩展。</p><p><strong>另外一类是一些方案提供商或者初创公司</strong>（比如Waymo，Mobileye，Pony.AI，Momenta，TuSimple等）。这些公司致力于发展L4级别的自动驾驶技术，面向的是诸如Robotaxi，Robotruck和Robobus之类的应用。</p><p><strong>对于不同的自动驾驶级别，不同的应用场景，传感器的配置方案也不尽相同。</strong>对于L2级别的应用，比如紧急制动和自适应巡航，可以只采用前视单目摄像头或者前向毫米波雷达。如果需要变道辅助功能，则需要增加传感器对相邻车道进行感知。常用的方案是在车头和车尾增加多个角雷达，以实现360度的目标检测能力。对于L3级别的应用，需要在特定场景下实现车辆的完全自主驾驶，因此需要扩展车辆对周边环境的感知能力。这时就需要增加激光雷达，侧视和后视的摄像头和毫米波雷达，以及GPS，IMU和高精度地图来辅助车辆定位。到了L4级别以后，由于在特定场景下不需要人工接管了，传感器就不仅需要高精确度，还需要高可靠性。这就需要增加传感器的冗余性，也就是说需要备用系统。</p><h2 id="概念及研究内容"><a href="#概念及研究内容" class="headerlink" title="概念及研究内容"></a>概念及研究内容</h2><p>自动驾驶车是一种通过计算机系统实现无人驾驶的智能汽车，是一个集合多领域先进技术的综合产物。<strong>感知、决策、执行</strong>是自动驾驶车的三大核心，使车辆能够感知和理解行驶环境，进行主动决策，对全局或局部地图进行实时地路径规划，并准确控制车辆运动，跟踪期望的轨迹，达到期望的行驶要求。</p><p>在智能车辆的研究中，主要涉及到以下的几个问题：（1）环境传感感知：该部分的主要作用是通过传感器识别车辆周围的障碍物与环境，建立车辆周围环境的三维场景；（2）轨迹规划与决策：该部分主要是在获得车辆周围环境后规划与决策出一条最优的行驶轨迹；（3）轨迹跟随控制：该部分主要是通过控制车辆的转向系统以及驱动制动系统使得车辆按照期望的轨迹行驶，包括了纵向的速度跟随控制以及侧向的路径跟随控制。</p><img src="https://s2.loli.net/2022/08/26/GpIuaMokxw6JKZB.jpg" alt="自动驾驶系统基本构成" style="zoom: 80%;" /><h3 id="环境传感感知"><a href="#环境传感感知" class="headerlink" title="环境传感感知"></a>环境传感感知</h3><p>在自动驾驶赛道中，感知的目的是为了模仿人眼采集相关信息，为后续做决策提供必要的信息。根据所做决策的任务不同，感知可以包括很多子任务：如<strong>车道线检测</strong>、<strong>3D目标检测</strong>、<strong>障碍物检测</strong>、<strong>红绿灯检测</strong>等等；再根据感知预测出的结果，完成决策；最后根据决策结果执行相应的操作（如<strong>减速、停车、变道、超车</strong>等）。</p><p>环境感知是实现自动驾驶的第一环节，也是最为关键的一个环节。车辆通过各类传感器，例如<strong>摄像头</strong>、<strong>毫米波雷达</strong>、<strong>超声波雷达</strong>、<strong>激光雷达</strong>等获取周边信息，产生图片数据、视频数据、点云图像、电磁波等信息，去除噪点信息后利用不同类型数据形成冗余同时提升感知精度，绘制周围区域的高精度3D地图。摄像头、雷达和激光雷达三者是相辅相成的关系。其中，<strong>摄像头</strong>是感知系统中最常用的传感器，优势在于能够提取丰富的纹理和颜色信息，因此适用于目标的分类。但是其缺点在于对于距离的感知能力较弱，并且受光照条件影响较大。<strong>激光雷达</strong>在一定程度上弥补了摄像头的缺点，可以精确的感知物体的距离和形状，因此适用于中近距的目标检测和测距。但是其缺点在于成本较高，量产难度大，感知距离有限，而且同样受天气影响较大。<strong>毫米波雷达</strong>具有全天候工作的特点，可以比较精确的测量目标的速度和距离，感知距离较远，价格也相对较低，因此适用于低成本的感知系统或者辅助其它的传感器。但是缺点在于高度和横向的分辨率较低，对于静止物体的感知能力有限。</p><img src="https://s2.loli.net/2022/08/26/RNv6sJQr2S3UjpK.png" alt="2021年1-5月国内新发布车型传感器配置及核心功能" style="zoom: 100%;" /><p>从硬件层面来看，对于不同级别自动驾驶汽车和驾驶任务而言，需要的传感器类型、数量和性能也有所区别。从软件算法层面来看，主要有视觉和点云的技术方案，比如特斯拉为代表的纯视觉；Waymo为代表的以点云和视觉融合的方案；还有以视觉为主融合激光点云的方案。</p><p>环境感知系统的硬件基础是多种传感器以及它们的组合，而软件方面的核心则是感知算法。总的来说，<strong>感知算法要完成两个主要的任务：物体检测和语义分割</strong>。前者得到的是场景中重要目标的信息，包括位置，大小，速度等，是一种稀疏的表示；而后者得到的是场景中每一个位置的语义信息，比如可行驶，障碍物等，是一种稠密的表示。<strong>这两个任务的结合被称为全景分割</strong>，这也是自动驾驶和机器人领域最近兴起的一个概念。对于物体目标（比如车辆，行人），全景分割输出其分割Mask，类别和实例ID；对于非物体目标（比如道路，建筑物），则只输出其分割Mask和类别。<strong>环境感知系统的终极目标就是要得到车辆周边三维空间中全景分割结果。</strong></p><p>自动驾驶驾驶技术这一轮的爆发很大程度上来源于深度学习在计算机视觉领域取得的突破，而这个突破首先是从图像分类和图像中的物体检测开始的。在自动驾驶环境感知中，深度学习最先取得应用的任务是<strong>单张二维图像中的物体检测。</strong>这个领域中的经典算法，比如Faster R-CNN，YOLO，CenterNet等都是不同时期视觉感知算法的主流。但是，车辆不能仅仅依靠一张二维图像上的检测结果来行驶。因此，为了满足自动驾驶应用的需求，这些基础的算法还需要进行进一步的扩展，其中最重要的就是<strong>融合时序信息和三维信息</strong>。前者衍生出了<strong>物体跟踪算法</strong>，后者衍生出了<strong>单目/双目/多目的三维物体检测算法</strong>。以此类推，语义分割包含了<strong>图像语义分割</strong>，<strong>视频语义分割</strong>，<strong>稠密深度估计</strong>。</p><p>为了得到更加精确的三维信息，激光雷达也一直是自动驾驶感知系统的重要组成部分，尤其是对于L3/4级别的应用。<strong>激光雷达的数据是相对稀疏的点云</strong>，这与图像稠密的网格结构差别非常大，因此图像领域常用的算法需要经过一定的改动才能应用到点云数据。<strong>点云感知的任务也可以按照物体检测和语义分割来划分，前者输出三维的物体边框，而后者输出点云中每个点的语义类别。</strong>为了利用图像领域的算法，点云可以转换为鸟瞰视图（Bird’s Eye View）或者前视图（Range View）下的稠密网格结构。此外，也可以改进深度学习中的卷积神经网络（Convolutional Neural Network, CNN），使其适用于稀疏的点云结构，比如PointNet或者Graph Neural Network。</p><p>毫米波雷达由于其全天候工作，测速准确，以及低成本的特点，也被广泛的用于自动驾驶感知系统中，不过一般应用在L2级别的系统中，或者在L3/4级系统中作为其它传感器的辅助。<strong>毫米波雷达的数据一般来说也是点云</strong>，但是比激光雷达的点云更为稀疏，空间分辨率也更低。<strong>相比于摄像头和激光雷达，毫米波雷达的数据密度非常低</strong>，因此一些传统方法（比如聚类和卡尔曼滤波）表现的并不比深度学习差很多，而这些传统方法的计算量相对较低。最近几年来，开始有研究者<strong>从更底层的数据出发，用深度学习代替经典的雷达信号处理，通过端对端的学习取得了近似激光雷达的感知效果</strong>。</p><p>单个传感器的感知能力总是有限的，如果把系统成本先放在一边，多传感器融合的方案自然更好的选择。一般来说，摄像头是感知系统的必备的传感器，为了得到深度信息和360度的视场，可以采用<strong>双目或者多目融合的方案</strong>。为了更准确的获得三维和运动信息，<strong>摄像头也可以与激光雷达和毫米波雷达进行融合</strong>。这些传感器的坐标系不同，数据形式不同，甚至采集频率也不同，因此融合算法的设计并不是一件简单的任务。粗略来说，<strong>融合可以在决策层（融合不同传感器的输出）或者数据层（融合不同传感器的数据或者中间结果）来进行</strong>。数据层融合理论上说是更好的方法，但是对<strong>传感器之间的空间和时间对齐</strong>要求会更高。</p><h3 id="轨迹规划与决策"><a href="#轨迹规划与决策" class="headerlink" title="轨迹规划与决策"></a>轨迹规划与决策</h3><p><strong>规划（planning）</strong>承接环境感知，并下启车辆控制。其规划出来的轨迹是带速度信息的路径。广义上，规划（planning）可分为<strong>路由寻径（Routing）、行为决策（Behavioral Decision）、运动规划（Motion Planning）</strong>。路由寻径：是全局路径规划，可简单的理解为传统地图导航+高精地图（包含车道信息和交通规则等）；行为决策：决策车辆是否跟车、在遇到交通灯和行人时的等待避让、以及路口和其他车辆的交互通过；运动规划：是局部路径规划，是无人车未来一段时间内的期望行驶路径，需满足汽车运动学、动力学、舒适性和无碰撞等要求。</p><p>轨迹规划的任务是计算出一个无碰撞可执行的轨迹（包含路径和速度信息），保证车辆从起点安全的驾驶到目的地，并尽可能高效。其问题的本质是一个多目标的数学优化问题。路径规划的底层数学理论主要是“最优化理论“”矩阵理论“”数值分析“，构建目标函数与约束条件同时求极值来得到最优控制量（路径）这个套路来自于”最优化理论“；在求解最优控制量时大家常见的牛顿法、最速下降法等等这些数值求解方法，本质来自于数值求解代数等式方程，属于”数值分析“；求解过程中所见到的导数雅可比矩阵、判断条件中的向量范数等等，本质就是把一维数值求解放到了高维，升维的方式由“矩阵理论”研究。</p><p>根据数学定义，横向、纵向规划均为非凸优化问题。横向规划，即s→(x,y)，决定了轨迹的形状。纵向规划是t→s，是指在此形状上运动的速度状态，也就是时间与位移的关系。横向规划和纵向规划联合起来就是t→(x,y)。</p><p><strong>决策（Decision）</strong>使轨迹规划化繁为简。既然轨迹规划是非凸优化问题，我们需要利用决策模块来解决这个问题。从数学上来讲，决策就是为了限定非凸问题（轨迹规划）的解空间，将问题转化为凸的。</p><p>主要的优化目标包括：</p><p><strong>安全性：</strong>避免与场景中的障碍物发生碰撞；针对动态障碍物，由于其未来运动的不确定性，降低其未来的碰撞风险；</p><p><strong>稳定性：</strong>由于车辆的惯性较大，灵活性差，期望轨迹需要保证车辆的物理可行性和控制器的稳定性；</p><p><strong>舒适性：</strong>考虑到乘员的舒适性，需要在满足安全性和稳定性的同时保证车辆的驾驶舒适度，包括加减速以及转向等过程；</p><p><strong>驾驶效率：</strong>在满足安全性和稳定性的同时，保证车辆以更快的速度驾驶，从而更短的时间到达目的地。</p><p>在实际场景中，规划过程需要考虑各种物理约束，有且不限于：</p><p><strong>加减速度约束：</strong>受到动力系统和制动系统的性能极限，及驾驶员的安全性和舒适性的制约；</p><p><strong>非完整性约束：</strong>车辆具有三个运动自由度，但是只有两个控制自由度，其非完整性约束决定了轨迹的物理可行性；</p><p><strong>动力学约束：</strong>考虑到车辆的动力学特性和车身稳定性，其驾驶过程中的曲率和横摆角速度具有一定的约束；</p><p>当然，在工程中，解决特定场景的规划问题，可以进行一定程度的简化。下面是几种应用较为广泛的算法。</p><img src="https://s2.loli.net/2022/08/26/RF8kdjGvePsI2Hx.jpg" alt="几种应用较为广泛的路径规划算法" style="zoom: 100%;" /><p>例如，百度的Apollo决策规划算法是一种<strong>数值优化的算法</strong>，且是“轻决策重规划”的方案，一共有3种Planner解决方案。</p><table><thead><tr><th align="left">EM Planner</th><th align="left">路径和速度解耦，分开规划再融合</th></tr></thead><tbody><tr><td align="left">RTK Planner</td><td align="left">基于录制的轨迹来规划行车的路线</td></tr><tr><td align="left">Lattice Planner</td><td align="left">路径和速度一起规划，直接输出高维的轨迹</td></tr></tbody></table><p>决策规划是智能汽车导航和控制的基础，从轨迹决策的角度考虑的，可分为<strong>全局规划和局部规划两个层次</strong>。其中，全局路径规划的任务是根据全局地图数据库信息规划出自起始点至目标点的一条无碰撞、可通过的路径。由于全局路径规划所生成的路径只能是从起始点到目标点的粗略路径，并没有考虑路径的方向、宽度、曲率、道路交叉以及路障等细节信息，加之智能汽车在行驶过程中受局部环境和自身状态的不确定性的影响，会遇到各种不可测的情况。因此，在智能汽车的行驶过程中，必须以局部环境信息和自身状态信息为基础，规划出一段无碰撞的理想局部路径，这就是局部路径规划。</p><h4 id="全局规划方法"><a href="#全局规划方法" class="headerlink" title="全局规划方法"></a><strong>全局规划方法</strong></h4><p><strong>(1) 基于状态空间的最优控制轨迹规划方法</strong></p><p><strong>最优控制一般包括一到两个性能指标</strong>，对于控制变量的取值不受约束的情况，一般用变分法进行求解；对于控制量受约束的情况，一般用极小值原理进行求解。</p><p><strong>(2)基于参数化曲线的轨迹规划方法</strong></p><p>B 样条曲线由一组称作控制点的向量来确定，这些控制点按顺序连接形成一个控制多边形，B样条曲线就是逼近这个控制多边形。<strong>通过确定控制点的位置，可以控制曲线的形状。</strong></p><p><strong>(3)基于基于系统特征的轨迹规划方法</strong></p><p>微分平坦法是基于系统特征的一种轨迹规划方法。微分平坦是指可以找到一组系统输出，使得所有状态变量和输入变量都可以由这组输出及其导数决定(不需积分)。</p><h4 id="局部规划方法（也可称之为实时路径规划）"><a href="#局部规划方法（也可称之为实时路径规划）" class="headerlink" title="局部规划方法（也可称之为实时路径规划）"></a><strong>局部规划方法（也可称之为实时路径规划）</strong></h4><p><strong>智能汽车进行局部路径规划（也可称之为实时路径规划）</strong>，一般是指在有障碍物的环境中，如何利用自身传感器感知周边环境，并寻找一条从当前点到目标点点的局部行驶路径，使智能汽车在本次任务中能安全快速地到达目标位置。局部路径规划的方法主要包括以下两个关键部分：</p><p><strong>建立环境模型，</strong>即将智能汽车所处现实世界抽象后，建立计算机可认知的环境模型；</p><p><strong>搜索无碰路径</strong>，即在某个模型的空间中，在多种约束条件下，选择合乎条件的路径搜索算法。根据不同行驶环境的特点，智能汽车局部路径规划中的侧重点和难点都会有相应不同：</p><ul><li><p>在高速公路中，行车环境比较简单但车速较快，此时对智能汽车控制精度要求很高，算法难点主要在于环境信息获取的位置精度和路径搜索的速度；</p></li><li><p>在城市半结构化道路中，道路环境特征性比较明显但交通环境比较复杂，周边障碍物较多，这就对智能汽车识别道路特征和障碍物的可靠性有较高要求，路径规划的难点主要在于车辆周边环境建模和避障行驶的路径搜索，特别是对动态障碍物方向和速度预测；</p></li><li><p>在越野环境的非结构化道路中，智能汽车所处的环境没有明显的道路边界，路面起伏不平，可能有大坑或土堆，这就对智能汽车识别周围环境，特别是地形地势有较高要求，路径规划的难点主要在于车辆可通行区域的识别。</p></li></ul><p><strong>(1)基于滚动时域优化的轨迹规划方法</strong></p><p>该方法能够确保机器人在未知环境中安全地避开障碍物行驶，具有反应速度快的优点，能够迅速适应变化的环境，是一种有效实用的工具，但计算量相对较大。</p><p><strong>(2)基于轨迹片段的运动规划方法</strong></p><p>轨迹片段包含配平轨迹和机动轨迹。其中配平轨迹是系统处于相对平衡时所经历的轨迹，而机动轨迹则是系统从一个相对平衡跃入另外一个相对平衡所经历的轨迹。可以通过考虑车辆的运动学和动力学约束条件，基于最优控制原理的机动轨迹设计方法和随机采样法，实现基于轨迹片段连接的最优运动轨迹规划和快速运动规划。但是该方法计算较为复杂，使其在实际应用中受到限制。</p><p><strong>(3)路权分配技术</strong></p><p>路权与车速强相关，可分为期望路权和实际路权，当两者不一致时，就需要进行调节来解决冲突。自主驾驶是智能汽车在任意时刻对路权的检测和使用，多车交互是车群在任意时刻对路权的竞争、占有、放弃等协同过程。<strong>自主驾驶的不确定性，体现在车辆行驶中拥有的路权在不停地发生变化。</strong></p><h3 id="轨迹跟随控制与执行"><a href="#轨迹跟随控制与执行" class="headerlink" title="轨迹跟随控制与执行"></a>轨迹跟随控制与执行</h3><p>自动驾驶控制执行系统是指系统做出决策规划以后，替代驾驶员对车辆进行控制，反馈到底层模块执行任务。可以说，执行控制系统是自动驾驶汽车行驶的基础，车辆的各个操控系统需要通过总线与决策系统相连接，并能够按照决策系统发出的总线指令精确地控制加速程度、制动程度、转向幅度、灯光控制等驾驶动作，以实现车辆的自主驾驶。</p><h4 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h4><p>自动驾驶控制执行的核心技术主要包括车辆的<strong>纵向控制</strong>和<strong>横向控制</strong>技术。纵向控制，即车辆的驱动与制动控制，是指通过对油门和制动的协调，实现对期望车速的精确跟随。横向控制，即通过方向盘角度的调整以及轮胎力的控制，实现自动驾驶汽车的路径跟踪。</p><p><strong>车辆纵向控制</strong>是指对车速以及本车与前后车或障碍物距离的自动控制。自动驾驶汽车采用油门和制动综合控制的方法来实现对预定车速的跟踪，各种电机-发动机-传动模型、汽车运行模型和刹车过程模型与不同的控制算法相结合，构成了各种各样的纵向控制模式。</p><img src="https://s2.loli.net/2022/08/26/qrh1L8nWtuR5KdE.webp" alt="典型的纵向控制系统结构（虚线框）" style="zoom: 100%;" /><p><strong>车辆横向控制</strong>指垂直于运动方向上的控制，即转向控制。横向控制系统目标是控制汽车自动保持期望的行车路线，并在不同的车速、载荷、风阻、路况下均有很好的乘坐舒适性和稳定性。</p><img src="https://s2.loli.net/2022/08/26/ZCESJXHpfwOxh1V.webp" alt="典型的横向控制系统结构（虚线框）" style="zoom: 100%;" /><p>车辆横向控制大致可以分为两种基本设计方法：<strong>基于驾驶员模拟</strong>的控制方法和<strong>基于车辆动力学模型</strong>的控制方法。</p><p>基于驾驶员模拟的方法又可以详细划分为两种，一种是使用较简单的动力学模型和驾驶员操纵规则设计控制器，另一种是用驾驶员操纵过程的数据训练控制器获取控制算法。</p><p>基于车辆动力学模型的方法，需要建立较精确的汽车横向运动模型。典型模型如单轨模型，该模型认为汽车左右两侧特性相同。</p><h4 id="控制算法"><a href="#控制算法" class="headerlink" title="控制算法"></a>控制算法</h4><p>自动驾驶控制方法可划分为两种，分别为传统控制方法与智能控制方法。</p><h5 id="传统控制方法"><a href="#传统控制方法" class="headerlink" title="传统控制方法"></a>传统控制方法</h5><p>传统控制方法主要有<strong>PID 控制</strong>、<strong>模糊控制</strong>、<strong>最优控制</strong>、**滑模控制(预测模型控制MPC)**等。</p><p>PID控制，又称之为比例积分微分控制，是最早发展起来的控制策略之一。其原理简单来说，是根据给定值和实际输出值构成控制偏差，将偏差按比例、积分和微分通过线性组合构成控制量，对被控对象进行控制。由于其算法简单、鲁棒性好和可靠性高，至今仍有90%左右的控制回路具有PID结构。</p><p>模糊控制，全称为模糊逻辑控制，是以模糊集合论、模糊语言变量和模糊逻辑推理为基础的一种计算机数字控制技术。与经典控制理论相比，模糊逻辑控制策略最大的特点是不需要准确的数学公式来建立被控对象的精确数学模型，因此可极大简化系统设计和数学建模的复杂性，提高系统建模和仿真控制的效率。不过，模糊控制的设计缺乏系统性，对复杂系统的控制是存在一定问题。</p><p>最优控制着重于研究使控制系统的性能指标实现最优化的基本条件和综合方法。可以概括为：对一个受控的动力学系统或运动过程，从一类允许的控制方案中找出一个最优的控制方案，使系统的运动在由某个初始状态转移到指定的目标状态的同时，其性能指标值为最优。</p><p>滑模控制也叫变结构控制，本质上是一类特殊的非线性控制。该控制策略与其他控制的不同之处在于系统“结构”不固定，可以在动态过程中根据系统当前的状态有目的地不断变化，迫使系统按照预定“滑动模态”的状态轨迹运动。由于滑动模态可以进行设计且与对象参数及扰动无关，因此滑动控制具有快速响应、对应参数变化及扰动不灵敏、无需系统在线辨识、物理实现简单等优点。不过，滑动控制也并不是全无缺点。在实际应用中，当状态轨迹到达滑动模态面后，难以严格沿着滑动模态面向平衡点滑动，而是在其两侧来回穿越地趋近平衡点，这会产生震动影响正常应用。</p><h5 id="智能控制方法"><a href="#智能控制方法" class="headerlink" title="智能控制方法"></a>智能控制方法</h5><p>智能控制方法与传统的控制方法最大的不同在于，智能控制方法更关注于控制对象模型的运用和综合信息学习运用。常见的智能控制方法主要有<strong>基于模型的控制</strong>、<strong>神经网络控制</strong>和<strong>深度学习方法</strong>等。</p><p>基于模型的控制，一般称为模型预测控制，它的当前控制动作是在每一个采样瞬间通过求解一个有限时域开环最优控制问题而获得。其基本原理可概括为：在每个采样时刻，根据当前获得的当前测量信息，在线求解一个有限时域的开环优化问题，并将得到的控制序列的第一个元素作用于被控对象，在一个采样时刻，重复上述过程，再用新的测量值刷新优化问题并重新求解。这种控制方法的优点是对模型的精度要求不高，建模方便，且因为采用非最小化描述的模型，系统鲁棒性、稳定性较好。</p><p>神经网络控制，可以把控制问题看成模式识别问题，被识别的模式映射成“行为”信号的“变化”信号。神经控制最显著的特点是具有学习能力。它是通过不断修正神经元之间的连接权值，并离散存储在连接网络中来实现的。它对非线性系统和难以建模的系统的控制具有良好效果。</p><p>深度学习方法，可以获得深层次的特征表示，免除人工选取特征的繁复冗杂和高维数据的维度灾难等问题，在特征提取与模型拟合方面具有很大优势。由于自动驾驶系统需要尽量减少人的参与，因此深度学习自动学习状态特征的能力，让深度学习在自动驾驶系统的研究中更具优势。</p><h4 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h4><p>根据从行驶环境到驾驶动作的映射过程，自动驾驶控制技术可以分为间接控制和直接控制两种不同方案。</p><p><strong>基于规划 - 跟踪的间接控制方法</strong></p><p>自动驾驶间接控制方案可以简单概括为，根据当前车辆行为需求，在满足车辆自身运动学和动力学约束条件下规划出一条空间上可行且时间上可控的无碰撞安全运动轨迹，然后设计适当的控制律跟踪生成的目标轨迹，从而实现自主驾驶。</p><img src="https://s2.loli.net/2022/08/26/DG5y2liZTVpKMjC.webp" alt="间接控制结构示意图" style="zoom: 100%;" /><p><strong>基于人工智能的直接控制方法</strong></p><p>由于自动驾驶汽车行驶环境具有不确定性、不可重复性和不可预测性等特征，很难建立精确的数学模型进行控制律的设计，因此传统控制策略已无法满足自动驾驶驾驶控制的要求。</p><p>这样的背景下，基于人工智能的直接控制方法就成为目前自动驾驶控制系统的主流形式。</p><img src="https://s2.loli.net/2022/08/26/uZHq2c5kSOFjlE6.webp" alt="直接控制结构示意图" style="zoom: 100%;" /><p>基于人工智能决策控制模型本质上是模拟人脑对外界环境信息和车体本身信息的感知，同时由驾驶经验并同在线学习机制来获得持续稳定输出的过程。</p><p>这种控制模式可以有效提升自动驾驶汽车在面对不同场景下的随机应变能力，代表着自动驾驶控制执行系统在未来一段时间内的主流发展方向。</p><hr><h2 id="引申与思考"><a href="#引申与思考" class="headerlink" title="引申与思考"></a>引申与思考</h2><p>近年来，应用于计算机视觉领域的深度学习技术得到了突破性的发展，随即被广泛应用于了自动驾驶领域。有大量的自动驾驶系统，包括 Apollo，Autoware等，使用了诸如物体检测、目标追踪、姿态预测等深度学习相关技术。深度学习的应用使得自动驾驶系统能够更加适应复杂的环境条件，更加适应复杂的任务需求；使自动驾驶逐渐从实验室走向现实，从辅助驾驶过渡到全自动驾驶。然而在另一方面，自动驾驶系统层出不穷的安全性问题引发了人们的担忧。深度学习系统自身具有的高度非线性、不可解释性特征，给系统的安全性分析提供了困难。目前已有大量的关于基于深度学习的自动驾驶系统的安全性保障、安全性测试的工作被提出，包括神经元覆盖、场景迁移覆盖、输入空间划分、故障注入检测、测试数据集生成等。</p><p>未来的挑战主要有几方面：</p><p>首先，在当今生物计算、脑科学尚未全面解锁的情况下，实证主义以“机器学习+大数据/复杂环境+大算力”模式训练大规模智能模型，确实可以解决不少问题，但是数据也面临着不少问题。首先数据需要大量的人力标注，人力标注意味着人也会出错，一旦标注错误，如何高效的发现错误将是一个关键的点，或者在标注的时候带有某些不和谐的色彩（性别、肤色等等），这都会影响算法模型的准确性公平公正性。</p><p>其次，算法模型的验证一般会采用开放数据集，但是这些数据集有一定的局限性，例如结构化场景的局限，Cornercase的不足等等，当模型精度到达一定的程度后就需要新的数据去训练和验证，那么这些数据的来源无非是通过实际测试或者虚拟仿真，虚拟仿真的工具开发将会是重点之一，这将会是进入数字世界的一把钥匙。</p><p>最后，算法再厉害也存在物理的限制，无论是哪种传感器都会有物理限制。根据信息论的观点，对抗不确定性的方法就是获取更多的相关信息。自动驾驶感知面对各种不确定，在硬件方面、软件方面同步获取信息，未来产品边界成熟在成本的压力下，将会选择最优化的方案，基于物理性质的感知将会是下一个重点方向，这就是汽车、手机行业开始跨界融合的原因之一。在视觉领域，无论是摄像头物理尺寸还是底层防抖、去反射、去雾等等，手机厂商已经有相对成熟的技术方案，随着汽车智能化的不断普及，手机行业和汽车行业的技术将会不断融合，具体就看融合的方式方法。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自动驾驶 </tag>
            
            <tag> 路径规划 </tag>
            
            <tag> 科研论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoboMaster视觉组学习整理（1）</title>
      <link href="/archives/b13dfb3f.html"/>
      <url>/archives/b13dfb3f.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=530397462&auto=1&height=66"></iframe></div>前面序章（0）提到了代码框架，我觉得直接按照框架顺序讲不便于梳理逻辑，所以打算按照算法流程来讲解各部分代码。<h1 id="算法流程介绍"><a href="#算法流程介绍" class="headerlink" title="算法流程介绍"></a>算法流程介绍</h1><p>先看下流程图：</p><img src="D:\MyBlog\MyBlog\source\img\算法流程图.png" alt="" style="zoom:50%;" /><p>（未完待续。。。）</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoboMaster视觉组学习整理（0）</title>
      <link href="/archives/25519.html"/>
      <url>/archives/25519.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=530397462&auto=1&height=66"></iframe></div><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>上学期一直在请教大佬和自学代码的路上，没有什么产出，想着这学期自己写点东西，就从RoboMaster这个比赛写起吧。我是研一上学期参加的比赛，做视觉识别部分。当时的想法很简单，因为很喜欢Linux，个人不太想调电控和焊板子接线，就参加了视觉部分。由于本科专业以及之前工作的关系，其实更熟悉机械结构设计，学视觉主要是因为兴趣，还有想换个方向。做视觉可以开发一些很有趣的东西，学到很多前沿的知识。</p><p>来到新战队之后百废待兴，可以说战队管理工作是我一手带起来的。今年我担任战队项目管理（兼任视觉组长）期间，也发现了一些现实问题：</p><p>一个是队员基础能力弱和经验缺乏带来的队伍上限不高。上学期各组真的是从零开始的——没技术没经验。虽然队里之前有研究生做，但是却没有相关技术传承，连个真正能解答问题的人都没有。例如上届视学组名义上是有几个人的，但由于各种原因都跑路了，只有一位学姐偶尔能说上话，但是很多时候她也无法及时帮忙解决问题。所以我很能理解一些新接触RM视觉的学弟学妹心中的迷茫；</p><p>一个是人员稀缺，目前视觉组只有我和另外一名研究生（之前还有个本科学弟的，前不久因为个人原因退队了……），现阶段再招人也不现实，而且参赛名额有限，所以暂时由我负责两个兵种。希望换届的时候，梯队这边能留下足够多人数；</p><p>所以我打算将RoboMaster视觉的各个方面的技术要点写成一系列的教程，帮助新手入门，也希望对其他战队有一点参考价值。教程代码主要参考了南理工本部Alliance战队上个赛季代码（感谢磊哥），传送门：<a href="https://github.com/shilei31415/AutoShot">https://github.com/shilei31415/AutoShot</a> 。本部往届的代码写得不错，搭建了一个很好的项目框架，很感谢他们能分享出来，帮助我们新队伍更快成型。</p><p>下面是代码框架，可以先看下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AutoShot</span><br><span class="line"> ├── AutoShot//识别装甲</span><br><span class="line"> │   ├── include//.h文件</span><br><span class="line"> │   │   ├── ANN.h//数字识别</span><br><span class="line"> │   │   ├── Armor.h//Mark匹配</span><br><span class="line"> │   │   ├── Mark.h//识别Mark</span><br><span class="line"> │   │   ├── pnp.h//PnP测距</span><br><span class="line"> │   │   ├── TargetFind.h//调用Armor和Mark</span><br><span class="line"> │   │   └── trajectory.h//考虑空气阻力的抛物线(未使用)</span><br><span class="line"> │   └── src//.cpp文件</span><br><span class="line"> │       ├── ANN.cpp</span><br><span class="line"> │       ├── Armor.cpp</span><br><span class="line"> │       ├── Mark.cpp</span><br><span class="line"> │       ├── pnp.cpp</span><br><span class="line"> │       ├── TargetFind.cpp</span><br><span class="line"> │       └── targetFinderTest.cpp//测试targetFinder</span><br><span class="line"> ├── CMakeLists.txt</span><br><span class="line"> ├── HeroVision.h//常用宏定义</span><br><span class="line"> ├── include.h//常用头文件</span><br><span class="line"> ├── main.cpp</span><br><span class="line"> ├── others//其他功能</span><br><span class="line"> │   ├── Calibrator//标定</span><br><span class="line"> │   │   ├── Calibrate.cpp</span><br><span class="line"> │   │   ├── Calibrator.cpp</span><br><span class="line"> │   │   ├── Calibrator.h</span><br><span class="line"> │   │   ├── cr</span><br><span class="line"> │   │   └── ip</span><br><span class="line"> │   ├── camera//相机驱动</span><br><span class="line"> │   │   ├── CameraApi.h</span><br><span class="line"> │   │   ├── CameraDefine.h</span><br><span class="line"> │   │   ├── CameraStatus.h</span><br><span class="line"> │   │   ├── MindVisionCamera.cpp</span><br><span class="line"> │   │   └── MindVisionCamera.h</span><br><span class="line"> │   ├── JLink//自己编写的调试助手,显示变量随时间的变化</span><br><span class="line"> │   │   ├── JLink.cpp</span><br><span class="line"> │   │   └── JLink.h</span><br><span class="line"> │   ├── picture//程序运行时保存的图片</span><br><span class="line"> │   ├── SerialPort//串口通信</span><br><span class="line"> │   │   ├── SerialPort.cpp</span><br><span class="line"> │   │   ├── SerialPort.h</span><br><span class="line"> │   │   ├── SerialPortTest.cpp</span><br><span class="line"> │   │   └── serial.sh</span><br><span class="line"> │   └── video//程序运行时保存的视频</span><br><span class="line"> ├── README.md</span><br><span class="line"> └── tools//运行及调试时常用函数</span><br><span class="line">     ├── excption//catch excption并保存到txt (未使用)</span><br><span class="line">     │   ├── myExcption.cpp</span><br><span class="line">     │   └── myExcption.h</span><br><span class="line">     ├── MathTool//常用数学函数</span><br><span class="line">     │   └── MathTool.h</span><br><span class="line">     ├── setter//读取参数</span><br><span class="line">     │   ├── model.pb</span><br><span class="line">     │   ├── para.yaml</span><br><span class="line">     │   ├── setter.cpp</span><br><span class="line">     │   └── setter.h</span><br><span class="line">     └── time//计算帧率,控制帧率</span><br><span class="line">         ├── timer.cpp</span><br><span class="line">         └── timer.h</span><br></pre></td></tr></table></figure><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p><strong>在讲代码之前先解答一些大家可能会有的疑问：</strong></p><h2 id="为什么要用Ubuntu系统"><a href="#为什么要用Ubuntu系统" class="headerlink" title="为什么要用Ubuntu系统"></a>为什么要用Ubuntu系统</h2><p><strong>跟 win 相比：</strong></p><ul><li>免费，安全；</li><li>很多软件可以通过 ppa 源下载，不需要跑到网站上下载，比较方便；</li><li>一般的 server 都使用 linux，所以省去了很多开发兼容性的问题；</li><li>shell 厉害；</li><li>占用空间比较小,可以安装在许多硬件上(TX2/NUC)；</li><li>运行速度比Windows快，运行程序时不像Windows需要调用其他许多东西，同样的程序Ubuntu上运行速度比Windows快10%左右；</li><li>权限高，Ubuntu上<strong>不存在</strong>卸不掉的流氓软件，知道密码甚至可把Ubuntu整个删掉;</li><li>……</li></ul><p><strong>跟其他 linux 相比：</strong></p><ul><li>UI 比较好看；</li><li>文档比较多，用的人比较多，有问题可以一起讨论；</li><li>针对 ubuntu 开发的软件比较多；</li><li>……</li></ul><h2 id="关于ubuntu的建议"><a href="#关于ubuntu的建议" class="headerlink" title="关于ubuntu的建议"></a>关于ubuntu的建议</h2><p>①在自己电脑安装Ubuntu时尽量多分配一些空间，用作平时学习建议至少50G。当然了，如果个人有开发软件的需求，或是跑大型项目、涉及很多图像视频识别时，应尽量分配更多空间，以便不时之需。</p><p>②安装的时候语言建议选择英文，可锻炼英语读写能力，也是因为之后的文件路径默认英文，使用中文路径容易出错。其实安装完成后可以修改语言，但要注意路径保持英文。</p><p>③基本命令要会，不用会太多，“增删改查”及一些常用命令就够了，另外记得安装完系统先换源 <a href="https://www.yisu.com/ask/5536.html">ubuntu更新源是什么意思 - 问答 - 亿速云 (yisu.com)</a> 。<br>“增”可以用命令新建文件、文件夹；<br>“删”可以用命令删除文件、清空文件夹；<br>“改”可以用命令修改文件权限、移动文件；<br>“查”可以用命令查看文件夹中的内容；<br>还有一些脚本语言(shell)，你起码可以用脚本让你的程序实现”开机自启动””程序意外终止后重启”；<br>其他还用很多需要掌握的，建议开始的时候时间较为充裕，尽量用命令，而不是GUI；</p><p>④当你配置完环境，装好程序，马上把你的系统转为镜像文件（虚拟机的话直接用快照功能即可），防止有一天“一切归0”</p><p>⑤最初上手可以安装Ubuntu16。因为16版本的教程比较多，当你熟悉后可以考虑使用18.04或者20.04（这两个版本相关的内容较少，如果报错你可能较难找到解决方法）</p><h2 id="配置OpenCV"><a href="#配置OpenCV" class="headerlink" title="配置OpenCV"></a>配置OpenCV</h2><h3 id="安装-OpenCV-contrib"><a href="#安装-OpenCV-contrib" class="headerlink" title="安装 OpenCV_contrib"></a>安装 OpenCV_contrib</h3><p>OpenCV3.0以上的版本，把一些不稳定的函数放到了第三方库OpenCV_contrib中（比如sift,surf等），为了应用这些功能，我们需要将Contrib中的功能重新加入Opencv。关于OpenCV_contrib，可以参见官方说明 <a href="https://github.com/opencv/opencv_contrib/blob/master/README.md">https://github.com/opencv/opencv_contrib/blob/master/README.md</a> </p><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>主要有两种方法：一种是使用CMake-gui这个图形化界面进行安装（适合对Ubuntu的命令不太熟悉的同学）；另一种是用终端直接输入命令操作，可以参考其他博客，这里就不再讨论。</p><p>顺带讲下环境配置的问题：看教程是一部分，配置多了也要有自己的思考，比如：为什么要这么做？环境变量的作用是什么？这样才能在之后各种环境配置中更加流畅。</p><h3 id="关于版本问题"><a href="#关于版本问题" class="headerlink" title="关于版本问题"></a><strong>关于版本问题</strong></h3><p>虽然目前已经更新到Opencv4，但我们依然可以用Opencv3，为什么？因为RM比赛一定要考虑做出来机器的稳定性，老版本可能不是最先进的，但比较之下是更稳定、更成熟的，如果说创新决定了一个团队的上限，那么稳定性则决定了一个团队的下限。</p><h2 id="安装CLion和PyCharm"><a href="#安装CLion和PyCharm" class="headerlink" title="安装CLion和PyCharm"></a>安装CLion和PyCharm</h2><p>CLion编写C++，PyCharm编写python(神经网络会用到)，python 需要注意配置环境，个人觉得不需要安装Anaconda，直接安装Python&gt;3.5，然后用pip配置环境(tensorflow或pytroch和opencv-py….)</p><p>软件安装不需要去找破解，直接注册学校的教育邮箱 <a href="https://blog.csdn.net/qq_40784315/article/details/116106252">https://blog.csdn.net/qq_40784315/article/details/116106252</a> ，然后绑定自己的 Jetbrains 账户即可免费使用。</p><h2 id="学会使用cmake"><a href="#学会使用cmake" class="headerlink" title="学会使用cmake"></a>学会使用cmake</h2><p>不要求cmake使用的多么6，但至少可以做到：不用CLion也可以编译简单C++程序，会编写CLion中的CMakeLists.txt文件。</p><img src="https://s2.loli.net/2022/02/26/v7zkNnK6uqVPWgd.jpg" alt="" style="zoom:25%;" /><h2 id="工业相机的选择"><a href="#工业相机的选择" class="headerlink" title="工业相机的选择"></a>工业相机的选择</h2><p>针对运动机器人和灯条识别，我们比赛用的工业相机一般选择较高帧率较低像素的配置。镜头焦距一般选用6或8mm。本届相机我们用的是Mindvision的，分辨率和帧率分别为1920*1200和165FPS，针对联盟赛，镜头焦距选的6mm（我们这批发过来的镜头尺寸明显比之前8mm大很多，但是老师那边代购说根据预算只能提供这款，下届视觉组可以提前写明需求买小尺寸的镜头，体积小重量轻显然对结构设计更有利）。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>若以后战队发展壮大，团队成员之间可能需要进行大量的文件传输，可以直接使用邮箱实现不同系统之间的传输，同时不用安装其他应用，不用考虑人在不在工作室，不用担心文件丢失。<br>学会使用各种学习资源：<a href="https://www.baidu.com/">Baidu</a>    <a href="https://www.csdn.net/">CSDN</a>    <a href="https://www.bilibili.com/">Bilibili</a>    <a href="https://github.com/">Github</a><br>视觉组同学如果有梯子，完全可以在github上进行代码迭代管理，进一步提升团队协作效率。</p><p>————————————————————-先说这么多，之后想到再补————————————————————- </p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉研究生建议</title>
      <link href="/archives/36880.html"/>
      <url>/archives/36880.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1356230976&auto=1&height=66"></iframe></div><p>细想了下，我是接触RM比赛以及计算机视觉后，才有了写博客的想法，所以打算先写点视觉相关的东西。在知乎看到几个回答还不错，这里记录一下，留作后面不断查看回顾。</p><h2 id="如何做好科研"><a href="#如何做好科研" class="headerlink" title="如何做好科研"></a>如何做好科研</h2><p>（1）无他，唯手熟尔，就是多读多写。论文就是越读越快的，我还是本科生的时候我导师告诉我刚入门时候要精读十篇好文章，我现在指导师弟师妹也是给他们挑好几篇文章让他们反复精读，起初水平不够的时候看论文很慢，经常抓不住重点都很正常不用急躁。读多了自然就越来越快了，后期看个摘要，看个插图甚至看个题目都明白大致内容了。关于coding就是多写，多去模仿，非计算机科班出身的我刚读博的时候代码很差，现在也不算好，但是这几年在学习别人的代码后也在慢慢提高，所以现在对于刚进实验室的师弟师妹我都对他们的代码有点要求，至少要做到模块化和复用性，现在github开源这么多有很多好的代码可以去模仿学习，跑跑开源代码多去写写，非常忌惮一个现象就是不动手去写，总是只跑开源代码，跑一个换一个框架，没有开源就什么也不会了，要动手自己去复现一些程序。</p><p>（2）good idea，这个我一直认可我导师的观点。我导师说有时候创新点不是整天天马行空想出来的，而是在你看了很多文章做了很多实验之后自然而然产生的，我的感觉也是这样，当你看了很多文章之后，你会发现解决A问题的思路可能在B问题上能够得到启发，当你做了很多实验之后你才能发现论文里隐藏的瑕疵点或者在某些场景下不适用，发现了问题很自然的就能想到一些解决的ideas。</p><p>（3）现在做视觉的人太多，全世界最聪明的一批人都在做cv dl ai，所以发表一篇好paper真的不简单，你的idea别的学术老油条们很容易就能想到，做的还比你漂亮，所以横向比较那些做理论方面的博士同学可能成果比你早比你多，一定要沉得下心保持自信持续努力，厚积而薄发。</p><p>（4）博士我认为还是要open一点，闭门造车一个人搞还是太难，多去接触接触大牛们的工作，多去听听别人的分享，并且也勇于分享自己的见解与工作，多与别人交流讨论，是有帮助的，多于一个博士而言，除了做出好的工作以外，writing 和presentation 的功力也非常重要，如何精确简洁的用文字描述你的工作，如何面对不同类型的听众来做一个合适的报告，都是很重要的技能，这些平时多思考，多学习，多尝试都能慢慢提高。</p><p>（5）努力与专注，做科研并不有趣，需要你自己内心对科研真心的喜欢，会自主的去关注最新的工作，会为自己做出的工作有自豪感，能够从研究工作找到乐趣，保持每天都努力，完成这场博士学位的马拉松。</p><p>（6）对于视觉而言，arxiv的cvpr版块是每天都必须关注的，cvpr iccv eccv三大顶会，pami ijcv等顶刊的论文必须熟知，iclr npis aaai等次相关的顶会也要关注。tf pytorch keras mxnet caffe等框架至少熟用一个，也要关注这些框架的更新动态。新智元，机器之心，paperweekly，ai研习社等等工作号也都可以关注，每天都会发布领域最新动态。旷视科技，商汤，微软亚研院，图森，依图，腾讯优图等等企业也都可以去申请实习，里面真的大牛太多，资源太好，会学到很多东西。</p><p>（7）愿你做有用的科研，做最好的工作。这里我非诚喜欢胡国圣老师在朋友圈发的三句话，希望他不要打我——喝最烈的威士忌,日最野的哈士奇,做最难的Research。</p><h2 id="微软亚洲研究院的回答"><a href="#微软亚洲研究院的回答" class="headerlink" title="微软亚洲研究院的回答"></a>微软亚洲研究院的回答</h2><p>作者：微软亚洲研究院<br>链接：<a href="https://www.zhihu.com/question/67257036/answer/487324016">https://www.zhihu.com/question/67257036/answer/487324016</a></p><p><strong>针对这个问题，在计算及视觉领域拥有17年的研究经历的微软首席研究员华刚博士有些话想和大家分享，他写下了这篇文章，希望能对计算机视觉领域内的学生和年轻的学者们有所帮助。</strong></p><p>简单说一下这篇文章的背景：从我2015年回到微软亚洲研究院之后接触到很多聪明的实习生。一方面感受到他们对计算机视觉研究的热情，另方面也有感于他们对计算机视觉研究认知的局限性，或者说大一点，是基本研究方法和思路上的局限性，就有想法要对如何做好计算机视觉的研究写点什么。计算机视觉领域国际权威、加州大学洛杉矶分校的朱松纯老师曾经发表一篇关于计算视觉的三个起源和人工智能的评论，全面深刻，引起了很大的反响。我想结合朱老师评论的内容，和我在计算及视觉领域17年的研究经历，也来谈谈如何做好计算机视觉的研究，希望对领域内的学生和年青的研究员能有所帮助。</p><p>“如何做好计算机视觉的研究？”要回答这个问题，我们先要对这个问题的关键词进行分析：如果去掉“计算机视觉”这个限定词，这问题就变成了“如何做好研究？”那么，要回答这个问题，我们就要知道“什么是好的研究？”</p><p>而要定义什么是好的研究，必须回到根本，先要知道：</p><h3 id="什么是研究？"><a href="#什么是研究？" class="headerlink" title="什么是研究？"></a>什么是研究？</h3><p>我们的讨论就从这个问题开始。什么是研究？一个被普遍接受的对研究的广义定义为：研究是为了产生新的知识或者是为已有的知识设计新的应用的系统性的工作。因为我们今天的讨论其实更多集中在科学研究上，先确定狭义的研究的定义为：利用科学的方法来调查解释一个现象或者获取新的知识。</p><p>综合这两个定义，可以看到科学研究从本质上是由三个基本的要素构成：1) 目的：产生新的知识或者是设计出新的应用; 2）手段：科学的方法。缺少这两个要素任何之一都不构成科学研究; 3) 成果：新的知识。所谓新的知识，必须是前人不知道的东西。</p><p>我们很多同学和年轻的研究员认为研究就是写论文、研究成果就是论文，这其实是在观念上走进了一个误区。论文是系统阐述新的知识、新的应用，以及阐述获取这个新知识或者新应用用到了什么样的科学方法的一个载体。论文，作为阐述研究成果的主要手段，必须经过同行的评议通过才能正式发表和被认可。</p><p>在人工智能进入第三个热潮之际，我们看到各种各样关于AI的各种媒体报道层出不穷，一方面，这对大众普及了AI各方面的知识，是积极的。但从另一个方面讲，很多观点没有经过仔细的推敲，也没有同行的评议，一些谬误或者是夸大的观点可能因为广泛传播而被大众接受，结果产生负面的社会影响。这就提醒我们相关领域的研究人员，在对大众媒体去做一些评论的时候，必须仔细斟酌，尽量不传播没有得到检验的观点。</p><p>这就谈到第二个问题：</p><h3 id="什么是好的研究？"><a href="#什么是好的研究？" class="headerlink" title="什么是好的研究？"></a>什么是好的研究？</h3><p>不同领域的研究员对这个问题可能会有不同的看法。</p><p>从计算机科学的角度来讲，尤其是计算机视觉的研究，无论是理论的还是实践的，我们的研究成果最终是要解决现实世界的问题的。在这个方面，我印象比较深刻的还是我在西安交通大学读研究生的时候，沈向洋博士2001年在西安交大做报告提到的一个观点：最好的研究员发现新问题；好的研究员创造新方法解好问题；一般的研究员跟随别人的方法解问题——大家在多次这里看到“新”这个关键词，创新是研究的本质。</p><p>有了这些铺垫，我们首先定义什么是最好的研究。通常认为一个领域中对于某一个问题最好的研究工作有三种：第一篇论文 (The First Paper)，最好的一篇论文 (The Best Paper)，以及最后一篇论文(The Last Paper)。这第一篇论文的含义是说这篇论文率先提出了一个好的问题和方向。最好的一篇论文是什么？那一定是开创性地提出了一种解法，启发了最终解决这个问题的途径。至于最后一篇论文，那一定是彻底把这个问题解决了，从此以后这个问题不再需要继续做进一步的研究。从计算机视觉领域举一个具体的例子来讲，Harris Corner Detector属于最早的一批在图像中检测角点的论文，可以归为第一篇之列。David Lowe博士的SIFT特征检测和局部描述子，可以归为在这个方向上最好的论文之列。那么这个方向的最后一篇呢？ 我认为可能还没有出现。具体到我自己的研究工作，在局部描述子这个方向上，我跟我的同事Matthew Brown和Simon Winder在2007年到2009年之间所做的一系列用机器学习的方法来建立描述子的工作，也实际上为提高局部描述子的性能提供了一个新的思路和方法。</p><p>对于我们很多研究员和学生来讲，一辈子可能都做不到这三种最好的研究工作之一。那是不是就等于说你不能做好的研究工作或者根本不用考虑做研究了呢？肯定不是这样。科学研究是一个共同体。这些最好的研究工作也是在前面很多很多非常扎实（solid）的研究工作的基础上发展出来的。因此，对于年青的研究员和学生而言，应该胸怀大志，去追求做最好的研究工作，但从实际执行上来讲，还是要把一项一项具体的工作先做扎实了。</p><p>怎么做到把研究工作做扎实了？首先，你必须对你要解的问题有一个全面深刻的了解，包括为什么要解这个问题、解这个问题有什么意义呢、以前有没有试图解决同样或者类似问题的先例，如果有，你就要全面了解前人都提出了什么样的解法、他们的解法都有什么样的优势和缺陷……最后，你的解法解决了前面这些解法不能解决的问题呢，或者是你的解法处理了什么样的他们不能处理的缺陷了？这些问题的答案如果都有了，那么，在写论文的过程中要注意的就是，1）你的假设是什么？2）你怎么验证了你的假设？这个验证既可以是理论上的证明，也可以是实验的验证。我们很多学生和年青的研究员，写论文的时候没有找到内在的逻辑关系，很多观点都是似是而非。或者说重一点，在论文撰写方面的训练严重不足。你的研究如果到了写论文的阶段，那就必须要有明确的观点提出来。这个观点必须明确无误，只有这样你才能被称为形成了新的知识。你的每一个观点都必须在理论上或者是实验中得到验证。另外，论文的撰写是为了让人看懂，不是让人看不懂，所以我们在撰写过程中必须尽量保证不去假设读者已经拥有了某些方面的知识。做好了这些，基本上你就有很大的可能性能够做出扎实（solid)的研究工作。</p><p>然后回到我们讨论的主题：</p><h3 id="如何做好计算机视觉的研究工作？"><a href="#如何做好计算机视觉的研究工作？" class="headerlink" title="如何做好计算机视觉的研究工作？"></a>如何做好计算机视觉的研究工作？</h3><p>其实，要回答这个问题，将我上面讲的所有观点加上“计算机视觉领域”这个限定词就行了。我这儿结合计算机视觉研究的一些现状及朱松纯老师的一些观点来进一步谈谈我的观点。</p><p>首先谈谈我观察到的一些现象。很多年轻的学生，现在讨论问题的时候都用这样的谈话：我发现用FC6层的特征，比用FC7层的特征，在某个图像数据集上比现在最好的算法提高了1.5%的识别精度，老师我们可以写论文了（如果大家不能理解这句话，FC6和FC7是表示AlexNet的两个中间输出层）。我想请问，你在这个过程中发现了什么样的普适的新的知识吗，又或者，在不是普适的情况下，你在什么限定条件下一定能够看到这样的识别精度提高了？</p><p>不错，提高识别精度是一个很好的目标，但要注意，计算机视觉的研究是要解决识别的问题，不是解某一个图像数据集。这些图像数据集提供了很好的验证你的假设和方法的手段，但如果你没有遵循科学的方法和和手段去设计你的算法和实验，你也不可能得到一个科学的结论，从而也不能产生新的知识，更不用谈对这个领域做出贡献。朱松纯老师在他的评论中提到，很多学生认为，计算机视觉现在就是调深度神经网络的参数，也就是说的这个问题。</p><p>所以，具体到对于刚开始从事计算机视觉研究的学生来讲，要做好这方面的研究，我觉得第一步还是要系统学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的来龙去脉、这个领域都有哪些基本的问题、哪些问题已经解得比较成熟而哪些问题还在初级阶段……这里，推荐所有的学生学习两本经典教材《Computer Vision: A Modern Approach》和《Computer Vision: Algorithms and Applications》，可以先读完第一本再读第二本。</p><p>只有对这个领域有了一个初步的全面了解，你才能够找到自己感兴趣的那个问题。在众多的问题当中，你是希望做三维重建，还是做图像识别、物体跟踪，又或是做计算摄影呢？做研究其实不是一个完全享乐的的过程，你必须要有足够的兴趣来保证你能持续地走下去，这在你感觉自己当前研究的思路走不下去的时候尤其具有重要意义。当你确定你感兴趣的问题，你应该首先全面调研一下这个问题的来龙去脉。这就意味着你不能只读过去五年的论文。你可以从过去一年的论文开始，慢慢追溯回到过去很久的相关的论文。有些时候，你会惊讶地发现前人想问题的深度。研究的英文单词是Research，拆开是Re-Search，用中文直译就是重新搜索和发现，而不是直接发现，其实就是说你要首先对这个问题做追本溯源。朱松纯老师提到的我们很多学生现在不读五年以前的论文，说的也是这个道理。</p><p>当你做好了这些，你必须钻进计算视觉的一个小的领域。人的精力是有限的，这就意味着你不可能把很多事情同时做好，所以在你选好方向之后，就要把你的精力集中在你感兴趣的一个问题上， 努力成为这个方面的专家。研究是一项长跑，很多时候，你在一个方向上比别人坚持久一点， 你就有机会超越他而成为某个方面的专家。最后，我也来谈谈深度学习对计算机视觉的影响。在这里，我对马里兰大学Rama Chellapa教授在Tom Huang教授80岁生日论坛上表达的观点非常认可，他认为，深度学习网络就像一个Pasta Machine：你把该放的东西放进去，它能给你产生好吃的Pasta。同时它也是一个Equalizer：无论你在计算机视觉领域有40年的经验还是0年的经验，只要你会用Caffee，你在一些问题，比方说图像识别上，都能产生差不多的结果。他开玩笑说这有点伤自尊 (It hurts my ego!)，但我们还是应该把它作为一个好的工具拥抱它。我想，他的言外之意，是我们的研究应该做得更深，要去理解这个工具为什么能够工作得比较好，从而产生新的知识去指导将来的研究和应用。</p><p>我认为，对于年轻的学生来讲，从深度学习的方法开始学习没有什么问题，但必须要进一步去了解一下其他的数学和算法工具，像统计贝叶斯的方法、优化的方法、信号处理的方法等等等的。计算机视觉的问题，其本质是不适定的反问题，解这一类问题需要多种方法的结合。这里面有深度学习解得比较好的问题，像图像识别，也有深度学习解不了的问题，像三维重建和识别。</p><p>任何研究领域包括计算机视觉的研究，对处在研究初期的学生而言， 更重要的是掌握足够的数学工具，培养一种正式思维（Formal Thinking）的能力，这样，遇到实际的问题就能以一种理论上正确的思路去解决这个问题。</p><p>作为结束语，我想对在从事或者有志于从事计算机视觉研究的学生说，计算机视觉的研究处在一个非常好的时期，有很多我们原来解不了的问题现在能够解得比较好了，像人脸识别，尽管我们其实还没有从真正意义上达到人类视觉系统对人脸识别的鲁棒程度。但我们离真正让计算机能够像人看和感知这个世界还有很远的距离。在我们达到这个目标之前，深度学习的方法可能是这个过程中一个重要的垫脚石，同时我们还要将更多的新的方法和工具带入这个领域来进一步推动这个领域的发展。</p><h3 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h3><p>华刚博士是微软首席研究员，研究重点是计算机视觉、模式识别、机器学习、人工智能和机器人，以及相关技术在云和移动智能领域的创新应用。他因在图像和视频中无限制环境人脸识别研究做出的突出贡献，于2015年被国际模式识别联合会（International Association on Pattern Recognition）授予”生物特征识别杰出青年研究员”奖励，因其在计算机视觉和多媒体研究方面的杰出贡献，于2016年被遴选为国际模式识别联合会院士（IAPR Fellow）和国际计算机联合会杰出科学家(ACM Distinguished Scientist) 。他担任过CVPR 2014/2017、ICCV 2011、ACM MM 2011/ 2012/ 2015/2017、ICIP 2012/2013/2015/2016、ICASSP 2012/ 2013等十多个顶级国际会议的领域主席，将担任CVPR 2019的程序主席。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 科研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/archives/16107.html"/>
      <url>/archives/16107.html</url>
      
        <content type="html"><![CDATA[<div align="middle">    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=16139397&auto=1&height=66"></iframe></div><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
